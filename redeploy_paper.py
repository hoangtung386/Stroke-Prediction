# -*- coding: utf-8 -*-
"""Redeploy_paper_gốc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ImFgYkmXPPAyob7TtaqJ5N-Lo2-CpJ-K
"""

import kagglehub
path = kagglehub.dataset_download("fedesoriano/stroke-prediction-dataset")

!pip install -q catboost ngboost

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from imblearn.over_sampling import BorderlineSMOTE
from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder
from sklearn.model_selection import (
    StratifiedKFold,
    train_test_split,
    RandomizedSearchCV
)
from sklearn.metrics import (accuracy_score, precision_score,
                             recall_score, f1_score, roc_auc_score,
                             confusion_matrix, roc_curve)
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import (
    RandomForestClassifier,
    GradientBoostingClassifier,
    BaggingClassifier
)
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from ngboost import NGBClassifier
from imblearn.ensemble import BalancedBaggingClassifier
from sklearn.ensemble import StackingClassifier, VotingClassifier
import joblib
import os

import warnings
warnings.filterwarnings('ignore')

SEED = 42
k_fold = 5

"""### Read dataset"""

data = pd.read_csv(f"{path}/healthcare-dataset-stroke-data.csv")
data.head()

"""### Data Overview"""

df = pd.DataFrame(data)

df.info()

# Nhận ra là cột BMI đang có missing value
df.describe()



df.describe(include='number').T

df.describe(include='object').T

df.head()

df.select_dtypes(include='number').columns

df.select_dtypes(include='object').columns

"""### Dataset Preprocessing"""

# Removing id column
df = df.drop(['id'], axis=1)

# Removing the rows that have "other" value
df = df.drop(df[df['gender'] == 'Other'].index)

# Cheking Duplicates
df.duplicated().sum()

# Checking Missing Value
print('The missing bmi value is:', df['bmi'].isnull().sum())

df['ever_married'] = df['ever_married'].map({'No': 0, 'Yes': 1})

categorical_cols = ['gender', 'work_type', 'Residence_type', 'smoking_status']
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

# Áp dụng One-Hot Encoding cho các cột categorical
encoded_features = encoder.fit_transform(df[categorical_cols])

# Tạo tên cột mới cho các feature encoded
new_feature_names = encoder.get_feature_names_out(categorical_cols)

# Chuyển encoded features trở lại DataFrame
encoded_df = pd.DataFrame(encoded_features, columns=new_feature_names, index=df.index)

# Xóa các cột categorical gốc khỏi df
df = df.drop(columns=categorical_cols)

# Concatenate the original df with the new encoded DataFrame
# Nối df gốc với DataFrame được encode
df = pd.concat([df, encoded_df], axis=1)

# Standard Normalization
numerical_cols = ['age', 'avg_glucose_level', 'bmi']

# Tính IQR (Interquartile Range)
Q1 = df[numerical_cols].quantile(0.25)
Q3 = df[numerical_cols].quantile(0.75)
IQR = Q3 - Q1

# Xác định ngưỡng lọc (Chọn 3 lần IQR - Extreme Outliers)
lower_bound = Q1 - 3 * IQR
upper_bound = Q3 + 3 * IQR

# Tạo điều kiện để giữ lại các sample đáp ứng yêu cầu
condition = ~((df[numerical_cols] < lower_bound) | (df[numerical_cols] > upper_bound)).any(axis=1)

df = df[condition].copy()

# Standard Normalization
scaler = StandardScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

df[:5]



"""## Drop missing value rows"""

# Tách các hàng không bị missing value
df_drop = df.copy()
df_drop = df_drop.dropna()

# Verify
print('The missing BMI value in df_drop is:', df_drop['bmi'].isnull().sum())

"""## Mean imputation"""

# Điền giá trị mean vào cột bmi
df_mean = df.copy()
df_mean['bmi'] = df['bmi'].fillna(df['bmi'].mean())

# Verify
print('The missing BMI value in bmi_fill_mean is:', df_mean['bmi'].isnull().sum())

"""## MICE Imputation"""

df_mice = df.copy()
mice_imputer = IterativeImputer(random_state=SEED, max_iter=10)

# Áp dụng MICE imputation cho cột bmi
df_mice['bmi'] = mice_imputer.fit_transform(df_mice[['bmi']])

# Verify
print(f'Mean value of BMI after MICE imputation: {df_mice["bmi"].mean():.2f}')

"""## Age Group-based Mean Imputation"""

df_age_group = df.copy()

age_col_index = 0   # ['age', 'avg_glucose_level', 'bmi']
mean_age_scaled = scaler.mean_[age_col_index]
std_age_scaled = scaler.scale_[age_col_index]

# Xác định khoảng độ tuổi
original_age_boundaries = [0, 20, 40, 60, 80, 100]

# Chuyển đổi các khoảng tuổi thành các giá trị tỷ lệ tương ứng
scaled_bins = [(val - mean_age_scaled) / std_age_scaled for val in original_age_boundaries]

print("Original Age Boundaries:", original_age_boundaries)
print("Corresponding Scaled Bins (for pd.cut):", scaled_bins)

# Tạo cột age_group dựa trên nhóm tuổi sử dụng scaled_bins
df_age_group['age_group'] = pd.cut(df_age_group['age'],
                                   bins=scaled_bins,
                                   labels=['0-20', '21-40', '41-60', '61-80', '81+'],
                                   include_lowest=True) # include_lowest để đảm bảo giá trị tối thiểu được gộp

# Điền giá trị mean theo nhóm tuổi cho các giá trị thiếu
df_age_group['bmi'] = df_age_group.groupby('age_group', observed=True)['bmi'].transform(
    lambda x: x.fillna(x.mean())
)

# Remove column 'age_group'
df_age_group = df_age_group.drop('age_group', axis=1)

df.shape

# Sử dụng các features quan trọng được xác định trong paper
important_features = ['age', 'bmi', 'avg_glucose_level', 'heart_disease', 'hypertension', 'ever_married', 'stroke']

# Tạo 3 imputed datasets với chỉ các features quan trọng
df_mean_important = df_mean[important_features].copy()
df_mice_important = df_mice[important_features].copy()
df_age_group_important = df_age_group[important_features].copy()

# Kiểm tra số lượng records trong mỗi dataset
print(f'Số records trong Mean imputed dataset: {len(df_mean_important)}')
print(f'Số records trong MICE imputed dataset: {len(df_mice_important)}')
print(f'Số records trong Age Group imputed dataset: {len(df_age_group_important)}')

"""## Augmented Dataset
Kết hợp 3 datasets trên để tạo ra
"""

# Concatenate 3 datasets vertically
augmented_dataset = pd.concat([df_mean_important,
                               df_mice_important,
                               df_age_group_important],
                              ignore_index=True)

print(f'Tổng số records trong Augmented Dataset trước khi loại duplicate: {len(augmented_dataset)}')

# Loại bỏ các dòng trùng lặp
augmented_dataset = augmented_dataset.drop_duplicates()

print(f'Số records trong Augmented Dataset sau khi loại duplicate: {len(augmented_dataset)}')

# Thông tin về Augmented Dataset
print(f'Shape: {augmented_dataset.shape}')
print(f'\nColumns: {list(augmented_dataset.columns)}')
print(f'\nThống kê mô tả:')
print(augmented_dataset.describe())

# Kiểm tra class imbalance trong augmented dataset
print("Phân bố nhãn stroke trong Augmented Dataset:")
stroke_counts = augmented_dataset['stroke'].value_counts()
print(stroke_counts)
print(f'\nTỷ lệ stroke cases: {(stroke_counts[1] / len(augmented_dataset) * 100):.2f}%')
print(f'Tỷ lệ non-stroke cases: {(stroke_counts[0] / len(augmented_dataset) * 100):.2f}%')

# Lưu Augmented Dataset (optional)
# augmented_dataset.to_csv('augmented_stroke_dataset.csv', index=False)
# print("Đã lưu Augmented Dataset vào file 'augmented_stroke_dataset.csv'")



"""# K-fold Cross Validation"""

# Setup K-FOLD Cross Validation
def evaluate_model_kfold(model, X, y, k=5, model_name="Model"):
    """
    Đánh giá model với k-fold cross validation
    """
    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=SEED)

    accuracy_scores = []
    precision_scores = []
    recall_scores = []
    f1_scores = []
    auc_scores = []

    fold = 1
    for train_idx, val_idx in skf.split(X, y):
        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]
        y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]

        # Train model
        model.fit(X_fold_train, y_fold_train)

        # Predict
        y_pred = model.predict(X_fold_val)
        y_pred_proba = model.predict_proba(X_fold_val)[:, 1] if hasattr(model, 'predict_proba') else y_pred

        # Calculate metrics
        accuracy_scores.append(accuracy_score(y_fold_val, y_pred))
        precision_scores.append(precision_score(y_fold_val, y_pred, zero_division=0))
        recall_scores.append(recall_score(y_fold_val, y_pred, zero_division=0))
        f1_scores.append(f1_score(y_fold_val, y_pred, zero_division=0))

        try:
            auc_scores.append(roc_auc_score(y_fold_val, y_pred_proba))
        except:
            auc_scores.append(0)

        fold += 1

    results = {
        'Model': model_name,
        'Accuracy': np.mean(accuracy_scores),
        'Precision': np.mean(precision_scores),
        'Recall': np.mean(recall_scores),
        'F1-Score': np.mean(f1_scores),
        'AUC': np.mean(auc_scores),
        'Accuracy_std': np.std(accuracy_scores)
    }

    return results

"""## Without Oversampling

### Drop Missing Value & Data Split
"""

# Tách features và target
X_drop = df_drop.drop('stroke', axis=1)
y_drop = df_drop['stroke']

# Data Split
X_train_drop, X_test_drop, y_train_drop, y_test_drop = train_test_split(
    X_drop, y_drop,
    test_size=0.3,
    random_state=SEED,
    stratify=y_drop
)

"""#### Baseline Model: Logistic Regression"""

baseline_model = LogisticRegression(random_state=SEED, max_iter=1000)
baseline_results = evaluate_model_kfold(
    baseline_model, X_train_drop, y_train_drop,
    k=k_fold, model_name="Baseline LR"
)

print("Baseline Model Results (K-Fold CV):")
for key, value in baseline_results.items():
    if key != 'Model':
        print(f"{key}: {value:.4f}")

# Test trên test set
baseline_model.fit(X_train_drop, y_train_drop)
y_pred_baseline = baseline_model.predict(X_test_drop)

print(f"Baseline Model Test Results:")
print(f"Accuracy: {accuracy_score(y_test_drop, y_pred_baseline):.4f}")
print(f"Precision: {precision_score(y_test_drop, y_pred_baseline, zero_division=0):.4f}")
print(f"Recall: {recall_score(y_test_drop, y_pred_baseline, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(y_test_drop, y_pred_baseline, zero_division=0):.4f}")



"""#### Advanced Classification Models"""

# Dictionary chứa các models
models = {
    'LR-AGD': LogisticRegression(solver='saga', max_iter=100, random_state=SEED),

    'Neural Network': MLPClassifier(
        hidden_layer_sizes=(24, 36, 48, 36, 24),  # Như paper mô tả
        random_state=SEED,
        max_iter=500
    ),

    'Random Forest': RandomForestClassifier(
        n_estimators=100,  # N_RF = 100 như paper
        random_state=SEED
    ),

    'Gradient Boosting': GradientBoostingClassifier(
        n_estimators=100,  # N_GB = 100 như paper
        random_state=SEED
    ),

    'CatBoost': CatBoostClassifier(
        iterations=100,  # N_CB = 100 như paper
        random_state=SEED,
        verbose=0
    ),

    'LightGBM': LGBMClassifier(
        n_estimators=100,  # N_LGBM = 100 như paper
        random_state=SEED,
        verbose=-1
    ),

    'XGBoost': XGBClassifier(
        n_estimators=100,  # N_XGB = 100 như paper
        random_state=SEED,
        eval_metric='logloss'
    ),

    'Balanced Bagging': BalancedBaggingClassifier(
        estimator=RandomForestClassifier(random_state=42),
        n_estimators=5,  # 5 Random Forest Classifiers như paper
        random_state=SEED
    ),

    'NGBoost': NGBClassifier(
        n_estimators=100,  # N_NGB = 100 như paper
        learning_rate=0.01,  # Như paper
        random_state=SEED,
        verbose=False
    )
}

# Train và evaluate tất cả models
all_results = []

for model_name, model in models.items():
    print('-' * 30)
    print(f"Training {model_name}")
    results = evaluate_model_kfold(model, X_train_drop, y_train_drop,
                                   k=k_fold, model_name=model_name)

    all_results.append(results)
    print(f"Accuracy: {results['Accuracy']:.4f}")
    print(f"Precision: {results['Precision']:.4f}")
    print(f"Recall: {results['Recall']:.4f}")
    print(f"F1-Score: {results['F1-Score']:.4f}\n")

# Thêm baseline vào kết quả
all_results.insert(0, baseline_results)

"""#### Analysis for best model"""

# Tạo DataFrame để so sánh
results_df = pd.DataFrame(all_results)
results_df = results_df.sort_values('Accuracy', ascending=False)

print("Xếp hạng Model Performance (theo Accuracy):")
print(results_df.to_string(index=False))

def visualize_results(results_df):
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # Accuracy comparison
    axes[0, 0].barh(results_df['Model'], results_df['Accuracy'], color='skyblue')
    axes[0, 0].set_xlabel('Accuracy')
    axes[0, 0].set_title('Model Accuracy Comparison')
    axes[0, 0].grid(axis='x', alpha=0.3)

    # Precision comparison
    axes[0, 1].barh(results_df['Model'], results_df['Precision'], color='lightgreen')
    axes[0, 1].set_xlabel('Precision')
    axes[0, 1].set_title('Model Precision Comparison')
    axes[0, 1].grid(axis='x', alpha=0.3)

    # Recall comparison
    axes[1, 0].barh(results_df['Model'], results_df['Recall'], color='salmon')
    axes[1, 0].set_xlabel('Recall')
    axes[1, 0].set_title('Model Recall Comparison')
    axes[1, 0].grid(axis='x', alpha=0.3)

    # F1-Score comparison
    axes[1, 1].barh(results_df['Model'], results_df['F1-Score'], color='gold')
    axes[1, 1].set_xlabel('F1-Score')
    axes[1, 1].set_title('Model F1-Score Comparison')
    axes[1, 1].grid(axis='x', alpha=0.3)

    plt.tight_layout()
    plt.show()

# Visualize kết quả
visualize_results(results_df)

# Xác định best model
best_model_name = results_df.iloc[0]['Model']
print(f"Model tốt nhất (theo Accuracy): {best_model_name}")

print(f"Accuracy: {results_df.iloc[0]['Accuracy']:.4f}")
print(f"Precision: {results_df.iloc[0]['Precision']:.4f}")
print(f"Recall: {results_df.iloc[0]['Recall']:.4f}")
print(f"F1-Score: {results_df.iloc[0]['F1-Score']:.4f}")

"""#### Fine tuning - Hyperparameter Optimization"""

# Lấy top 3 models
top_3_models = results_df.head(3)['Model'].tolist()

# Định nghĩa parameter grids cho từng model
param_grids = {
    'Random Forest': {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    },
    'XGBoost': {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 5, 7],
        'learning_rate': [0.01, 0.1, 0.3],
        'subsample': [0.8, 0.9, 1.0]
    },
    'LightGBM': {
        'n_estimators': [100, 200, 300],
        'max_depth': [5, 10, 15],
        'learning_rate': [0.01, 0.1, 0.2],
        'num_leaves': [31, 50, 70]
    },
    'LR-AGD': {
        'C': [0.001, 0.01, 0.1, 1, 10],
        'max_iter': [100, 200, 500]
    },
    'Gradient Boosting': {
        'n_estimators': [100, 200],
        'learning_rate': [0.01, 0.1],
        'max_depth': [3, 5, 7]
    }
}

# Fine tuning top 3 models
tuned_models = {}

for model_name in top_3_models:
    if model_name in param_grids and model_name in models:
        print(f"Fine-tuning {model_name}")

        random_search = RandomizedSearchCV(
            models[model_name],
            param_grids[model_name],
            n_iter=10,
            cv=3,
            scoring='f1',
            random_state=SEED,
            n_jobs=-1
        )

        random_search.fit(X_train_drop, y_train_drop)
        tuned_models[model_name] = random_search.best_estimator_

        print(f"Best parameters: {random_search.best_params_}")
        print(f"Best CV score: {random_search.best_score_:.4f}")

print(f"\nFine-tuning completed for {len(tuned_models)} models!")

"""#### Dense Stacking Ensemble (DSE) Model

**Building Dense Stacking Ensemble (DSE) Model**
"""

# Sử dụng các tuned models hoặc original models
base_models_for_ensemble = []

for model_name, model in models.items():
    if model_name in tuned_models:
        base_models_for_ensemble.append((model_name, tuned_models[model_name]))
    else:
        base_models_for_ensemble.append((model_name, model))

# Xác định meta-classifier (best model)
# Theo paper, Random Forest thường là best performer
meta_classifier = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier = tuned_models[best_model_name]

print(f"Meta-classifier: {best_model_name}")
print(f"Base models: {len(base_models_for_ensemble)}")

# 1. VOTING ENSEMBLE
print("--- Building Voting Ensemble ---")

# Create a list of estimators specifically for VotingClassifier, excluding NGBoost
# as it seems to be causing a compatibility issue with sklearn's VotingClassifier.
base_models_for_voting = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

voting_ensemble = VotingClassifier(
    estimators=base_models_for_voting,
    voting='soft'
)

voting_ensemble.fit(X_train_drop, y_train_drop)
voting_pred = voting_ensemble.predict(X_test_drop)

print("Evaluation of the Voting Ensemble")
print(f"Accuracy: {accuracy_score(y_test_drop, voting_pred):.4f}")
print(f"Precision: {precision_score(y_test_drop, voting_pred):.4f}")
print(f"Recall: {recall_score(y_test_drop, voting_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_drop, voting_pred):.4f}")

# 2. BLENDING ENSEMBLE (Stacking with meta-classifier)
print("--- Building Blending Ensemble ---")

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

blending_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier,
    cv=5
)

blending_ensemble.fit(X_train_drop, y_train_drop)
blending_pred = blending_ensemble.predict(X_test_drop)

print("Evaluation of the Blending Ensemble")
print(f"Accuracy: {accuracy_score(y_test_drop, blending_pred):.4f}")
print(f"Precision: {precision_score(y_test_drop, blending_pred):.4f}")
print(f"Recall: {recall_score(y_test_drop, blending_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_drop, blending_pred):.4f}")

# 3. FUSION ENSEMBLE (Combining base models and best model)
print("--- Building Fusion Ensemble ---")
# Train meta-classifier riêng
meta_classifier_fusion = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier_fusion = tuned_models[best_model_name]

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

fusion_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier_fusion,
    cv=5,
    passthrough=True
)

fusion_ensemble.fit(X_train_drop, y_train_drop)
fusion_pred = fusion_ensemble.predict(X_test_drop)

print("Evaluation of the Fusion Ensemble")
print(f"Accuracy: {accuracy_score(y_test_drop, fusion_pred):.4f}")
print(f"Precision: {precision_score(y_test_drop, fusion_pred):.4f}")
print(f"Recall: {recall_score(y_test_drop, fusion_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_drop, fusion_pred):.4f}")

# 4. DENSE STACKING ENSEMBLE (DSE) - Final Model
print("--- Building Dense Stacking Ensemble (DSE) ---")

# DSE combines all three approaches
dse_base_models = [
    ('voting', voting_ensemble),
    ('blending', blending_ensemble),
    ('fusion', fusion_ensemble)
]

dse_model = StackingClassifier(
    estimators=dse_base_models,
    final_estimator=meta_classifier,
    cv=5
)

dse_model.fit(X_train_drop, y_train_drop)



"""#### Stroke prediction: Final Evaluation"""

# Predictions
y_pred_dse = dse_model.predict(X_test_drop)
y_pred_proba_dse = dse_model.predict_proba(X_test_drop)[:, 1]

# Calculate metrics
dse_accuracy = accuracy_score(y_test_drop, y_pred_dse)
dse_precision = precision_score(y_test_drop, y_pred_dse, zero_division=0)
dse_recall = recall_score(y_test_drop, y_pred_dse, zero_division=0)
dse_f1 = f1_score(y_test_drop, y_pred_dse, zero_division=0)
dse_auc = roc_auc_score(y_test_drop, y_pred_proba_dse)

print("DSE MODEL PERFORMANCE:")
print(f"Accuracy:  {dse_accuracy:.4f} ({dse_accuracy*100:.2f}%)")
print(f"Precision: {dse_precision:.4f} ({dse_precision*100:.2f}%)")
print(f"Recall:    {dse_recall:.4f} ({dse_recall*100:.2f}%)")
print(f"F1-Score:  {dse_f1:.4f} ({dse_f1*100:.2f}%)")
print(f"AUC:       {dse_auc:.4f} ({dse_auc*100:.2f}%)")

# Confusion Matrix
cm = confusion_matrix(y_test_drop, y_pred_dse)
print("Confusion Matrix:")
print(cm)

def visualize_confusion_matrix(cm):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No Stroke', 'Stroke'],
                yticklabels=['No Stroke', 'Stroke'])
    plt.title('DSE Model - Confusion Matrix')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

def plot_roc_curve(y_test_drop, y_pred_proba_dse, dse_auc):
    fpr, tpr, thresholds = roc_curve(y_test_drop, y_pred_proba_dse)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {dse_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('DSE Model - ROC Curve')
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)
    plt.show()

visualize_confusion_matrix(cm)

plot_roc_curve(y_test_drop, y_pred_proba_dse, dse_auc)

"""#### Feature Importance Analysis"""

# Lấy feature importance từ meta-classifier nếu có
if hasattr(dse_model.final_estimator_, 'feature_importances_'):
    # Get predictions from base models
    base_predictions = []
    for name, model in dse_base_models:
        pred = model.predict_proba(X_train_drop)
        base_predictions.append(pred)

    # Feature importance từ final estimator
    importances = dse_model.final_estimator_.feature_importances_

    # Create feature names
    feature_names = [f"{name}_0" for name, _ in dse_base_models] + \
                   [f"{name}_1" for name, _ in dse_base_models]

    # Sort by importance
    indices = np.argsort(importances)[::-1]

    print("Top Features (from ensemble):")
    for i in range(min(10, len(indices))):
        print(f"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}")

# Feature importance từ original features (using Random Forest)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_drop, y_train_drop)

feature_importance = pd.DataFrame({
    'Feature': X_train_drop.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

print("Original Feature Importance (Random Forest):")
print(feature_importance)

# Visualize
plt.figure(figsize=(5, 3))
plt.barh(feature_importance['Feature'][:10], feature_importance['Importance'][:10])
plt.xlabel('Importance')
plt.title('Top 10 Most Important Features')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# So sánh với tốt nhất của paper: 96,59% (Bộ dữ liệu cân bằng)
print("Model DSE đạt được:")
print(f"Accuracy: {dse_accuracy*100:.2f}%")
print(f"Precision: {dse_precision:.4f}")
print(f"Recall: {dse_recall:.4f}")
print(f"F1-Score: {dse_f1:.4f}")
print(f"AUC: {dse_auc*100:.2f}%")

"""#### Save the final model with the data removed for missing values."""

# Tạo folder lưu model
folder_name = 'Model for Drop Missing Value Imbalanced'
os.makedirs(folder_name, exist_ok=True)

# Lưu model
model_filename = 'dse_stroke_prediction_imbalanced_drop.pkl'

model_path = os.path.join(folder_name, model_filename)
joblib.dump(dse_model, model_path)
print(f"Model saved at: '{model_path}'")

# Lưu scaler
scaler_filename = 'scaler_imbalanced_drop.pkl'
scaler_path = os.path.join(folder_name, scaler_filename)
joblib.dump(scaler, scaler_path)
print(f"Scaler saved at: '{scaler_path}'")

# Lưu encoder
encoder_filename = 'encoder_imbalanced_drop.pkl'
encoder_path = os.path.join(folder_name, encoder_filename)
joblib.dump(encoder, encoder_path)
print(f"Encoder saved at: '{encoder_path}'")

# Lưu danh sách các cột feature
model_columns_filename = 'model_columns_imbalanced_drop.pkl'
columns_path = os.path.join(folder_name, model_columns_filename)
joblib.dump(X_train_drop.columns, columns_path)
print(f"Model columns saved at: '{columns_path}'")



"""### Mean imputation & Data Split"""

# Tách features và target
X_mean = df_mean.drop('stroke', axis=1)
y_mean = df_mean['stroke']

# Data Split
X_train_mean, X_test_mean, y_train_mean, y_test_mean = train_test_split(
    X_mean, y_mean,
    test_size=0.3,
    random_state=SEED,
    stratify=y_mean
)

"""#### Baseline Model: Logistic Regression"""

baseline_model = LogisticRegression(random_state=SEED, max_iter=1000)
baseline_results = evaluate_model_kfold(
    baseline_model, X_train_mean, y_train_mean,
    k=k_fold, model_name="Baseline LR"
)

print("Baseline Model Results (K-Fold CV):")
for key, value in baseline_results.items():
    if key != 'Model':
        print(f"{key}: {value:.4f}")

# Test trên test set
baseline_model.fit(X_train_mean, y_train_mean)
y_pred_baseline = baseline_model.predict(X_test_mean)

print(f"Baseline Model Test Results:")
print(f"Accuracy: {accuracy_score(y_test_mean, y_pred_baseline):.4f}")
print(f"Precision: {precision_score(y_test_mean, y_pred_baseline, zero_division=0):.4f}")
print(f"Recall: {recall_score(y_test_mean, y_pred_baseline, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(y_test_mean, y_pred_baseline, zero_division=0):.4f}")



"""#### Advanced Classification Models"""

# Train và evaluate tất cả models
all_results = []

for model_name, model in models.items():
    print('-' * 30)
    print(f"Training {model_name}")
    results = evaluate_model_kfold(model, X_train_mean, y_train_mean,
                                   k=k_fold, model_name=model_name)

    all_results.append(results)
    print(f"Accuracy: {results['Accuracy']:.4f}")
    print(f"Precision: {results['Precision']:.4f}")
    print(f"Recall: {results['Recall']:.4f}")
    print(f"F1-Score: {results['F1-Score']:.4f}\n")

# Thêm baseline vào kết quả
all_results.insert(0, baseline_results)

"""#### Analysis for best model"""

# Tạo DataFrame để so sánh
results_df = pd.DataFrame(all_results)
results_df = results_df.sort_values('Accuracy', ascending=False)

print("Xếp hạng Model Performance (theo Accuracy):")
print(results_df.to_string(index=False))

# Visualize kết quả
visualize_results(results_df)

# Xác định best model
best_model_name = results_df.iloc[0]['Model']
print(f"Model tốt nhất (theo Accuracy): {best_model_name}")

print(f"Accuracy: {results_df.iloc[0]['Accuracy']:.4f}")
print(f"Precision: {results_df.iloc[0]['Precision']:.4f}")
print(f"Recall: {results_df.iloc[0]['Recall']:.4f}")
print(f"F1-Score: {results_df.iloc[0]['F1-Score']:.4f}")

"""#### Fine tuning - Hyperparameter Optimization"""

# Lấy top 3 models
top_3_models = results_df.head(3)['Model'].tolist()

# Fine tuning top 3 models
tuned_models = {}

for model_name in top_3_models:
    if model_name in param_grids and model_name in models:
        print(f"Fine-tuning {model_name}")

        random_search = RandomizedSearchCV(
            models[model_name],
            param_grids[model_name],
            n_iter=10,
            cv=3,
            scoring='f1',
            random_state=SEED,
            n_jobs=-1
        )

        random_search.fit(X_train_mean, y_train_mean)
        tuned_models[model_name] = random_search.best_estimator_

        print(f"Best parameters: {random_search.best_params_}")
        print(f"Best CV score: {random_search.best_score_:.4f}")

print(f"\nFine-tuning completed for {len(tuned_models)} models!")

"""#### Dense Stacking Ensemble (DSE) Model

**Building Dense Stacking Ensemble (DSE) Model**
"""

# Sử dụng các tuned models hoặc original models
base_models_for_ensemble = []

for model_name, model in models.items():
    if model_name in tuned_models:
        base_models_for_ensemble.append((model_name, tuned_models[model_name]))
    else:
        base_models_for_ensemble.append((model_name, model))

# Xác định meta-classifier (best model)
# Theo paper, Random Forest thường là best performer
meta_classifier = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier = tuned_models[best_model_name]

print(f"Meta-classifier: {best_model_name}")
print(f"Base models: {len(base_models_for_ensemble)}")

# 1. VOTING ENSEMBLE
print("--- Building Voting Ensemble ---")

# Create a list of estimators specifically for VotingClassifier, excluding NGBoost
# as it seems to be causing a compatibility issue with sklearn's VotingClassifier.
base_models_for_voting = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

voting_ensemble = VotingClassifier(
    estimators=base_models_for_voting,
    voting='soft'
)

voting_ensemble.fit(X_train_mean, y_train_mean)
voting_pred = voting_ensemble.predict(X_test_mean)

print("Evaluation of the Voting Ensemble")
print(f"Accuracy: {accuracy_score(y_test_mean, voting_pred):.4f}")
print(f"Precision: {precision_score(y_test_mean, voting_pred):.4f}")
print(f"Recall: {recall_score(y_test_mean, voting_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_mean, voting_pred):.4f}")

# 2. BLENDING ENSEMBLE (Stacking with meta-classifier)
print("--- Building Blending Ensemble ---")

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

blending_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier,
    cv=5
)

blending_ensemble.fit(X_train_mean, y_train_mean)
blending_pred = blending_ensemble.predict(X_test_mean)

print("Evaluation of the Blending Ensemble")
print(f"Accuracy: {accuracy_score(y_test_mean, blending_pred):.4f}")
print(f"Precision: {precision_score(y_test_mean, blending_pred):.4f}")
print(f"Recall: {recall_score(y_test_mean, blending_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_mean, blending_pred):.4f}")

# 3. FUSION ENSEMBLE (Combining base models and best model)
print("--- Building Fusion Ensemble ---")
# Train meta-classifier riêng
meta_classifier_fusion = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier_fusion = tuned_models[best_model_name]

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

fusion_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier_fusion,
    cv=5,
    passthrough=True
)

fusion_ensemble.fit(X_train_mean, y_train_mean)
fusion_pred = fusion_ensemble.predict(X_test_mean)

print("Evaluation of the Fusion Ensemble")
print(f"Accuracy: {accuracy_score(y_test_mean, fusion_pred):.4f}")
print(f"Precision: {precision_score(y_test_mean, fusion_pred):.4f}")
print(f"Recall: {recall_score(y_test_mean, fusion_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_mean, fusion_pred):.4f}")

# 4. DENSE STACKING ENSEMBLE (DSE) - Final Model
print("--- Building Dense Stacking Ensemble (DSE) ---")

# DSE combines all three approaches
dse_base_models = [
    ('voting', voting_ensemble),
    ('blending', blending_ensemble),
    ('fusion', fusion_ensemble)
]

dse_model = StackingClassifier(
    estimators=dse_base_models,
    final_estimator=meta_classifier,
    cv=5
)

dse_model.fit(X_train_mean, y_train_mean)



"""#### Stroke prediction: Final Evaluation"""

# Predictions
y_pred_dse = dse_model.predict(X_test_mean)
y_pred_proba_dse = dse_model.predict_proba(X_test_mean)[:, 1]

# Calculate metrics
dse_accuracy = accuracy_score(y_test_mean, y_pred_dse)
dse_precision = precision_score(y_test_mean, y_pred_dse, zero_division=0)
dse_recall = recall_score(y_test_mean, y_pred_dse, zero_division=0)
dse_f1 = f1_score(y_test_mean, y_pred_dse, zero_division=0)
dse_auc = roc_auc_score(y_test_mean, y_pred_proba_dse)

print("DSE MODEL PERFORMANCE:")
print(f"Accuracy:  {dse_accuracy:.4f} ({dse_accuracy*100:.2f}%)")
print(f"Precision: {dse_precision:.4f} ({dse_precision*100:.2f}%)")
print(f"Recall:    {dse_recall:.4f} ({dse_recall*100:.2f}%)")
print(f"F1-Score:  {dse_f1:.4f} ({dse_f1*100:.2f}%)")
print(f"AUC:       {dse_auc:.4f} ({dse_auc*100:.2f}%)")

# Confusion Matrix
cm = confusion_matrix(y_test_mean, y_pred_dse)
print("Confusion Matrix:")
print(cm)

visualize_confusion_matrix(cm)

plot_roc_curve(y_test_mean, y_pred_proba_dse, dse_auc)

"""#### Save the final model with the data removed for missing values."""

# Tạo folder lưu model
folder_name = 'Model for Mean Imputation'
os.makedirs(folder_name, exist_ok=True)

# Lưu model
model_filename = 'dse_stroke_prediction_imbalanced_mean.pkl'

model_path = os.path.join(folder_name, model_filename)
joblib.dump(dse_model, model_path)
print(f"Model saved at: '{model_path}'")

# Lưu scaler
scaler_filename = 'scaler_imbalanced_mean.pkl'
scaler_path = os.path.join(folder_name, scaler_filename)
joblib.dump(scaler, scaler_path)
print(f"Scaler saved at: '{scaler_path}'")

# Lưu encoder
encoder_filename = 'encoder_imbalanced_mean.pkl'
encoder_path = os.path.join(folder_name, encoder_filename)
joblib.dump(encoder, encoder_path)
print(f"Encoder saved at: '{encoder_path}'")

# Lưu danh sách các cột feature
model_columns_filename = 'model_columns_imbalanced_mean.pkl'
columns_path = os.path.join(folder_name, model_columns_filename)
joblib.dump(X_train_drop.columns, columns_path)
print(f"Model columns saved at: '{columns_path}'")

"""### MICE imputation & Data Split"""

# Tách features và target
X_mice = df_mice.drop('stroke', axis=1)
y_mice = df_mice['stroke']

# Data Split
X_train_mice, X_test_mice, y_train_mice, y_test_mice = train_test_split(
    X_mice, y_mice,
    test_size=0.3,
    random_state=SEED,
    stratify=y_mice
)

"""#### Baseline Model: Logistic Regression"""

baseline_model = LogisticRegression(random_state=SEED, max_iter=1000)
baseline_results = evaluate_model_kfold(
    baseline_model, X_train_mice, y_train_mice,
    k=k_fold, model_name="Baseline LR"
)

print("Baseline Model Results (K-Fold CV):")
for key, value in baseline_results.items():
    if key != 'Model':
        print(f"{key}: {value:.4f}")

# Test trên test set
baseline_model.fit(X_train_mice, y_train_mice)
y_pred_baseline = baseline_model.predict(X_test_mice)

print(f"Baseline Model Test Results:")
print(f"Accuracy: {accuracy_score(y_test_mice, y_pred_baseline):.4f}")
print(f"Precision: {precision_score(y_test_mice, y_pred_baseline, zero_division=0):.4f}")
print(f"Recall: {recall_score(y_test_mice, y_pred_baseline, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(y_test_mice, y_pred_baseline, zero_division=0):.4f}")



"""#### Advanced Classification Models"""

# Train và evaluate tất cả models
all_results = []

for model_name, model in models.items():
    print('-' * 30)
    print(f"Training {model_name}")
    results = evaluate_model_kfold(model, X_train_mice, y_train_mice,
                                   k=k_fold, model_name=model_name)

    all_results.append(results)
    print(f"Accuracy: {results['Accuracy']:.4f}")
    print(f"Precision: {results['Precision']:.4f}")
    print(f"Recall: {results['Recall']:.4f}")
    print(f"F1-Score: {results['F1-Score']:.4f}\n")

# Thêm baseline vào kết quả
all_results.insert(0, baseline_results)

"""#### Analysis for best model"""

# Tạo DataFrame để so sánh
results_df = pd.DataFrame(all_results)
results_df = results_df.sort_values('Accuracy', ascending=False)

print("Xếp hạng Model Performance (theo Accuracy):")
print(results_df.to_string(index=False))

# Visualize kết quả
visualize_results(results_df)

# Xác định best model
best_model_name = results_df.iloc[0]['Model']
print(f"Model tốt nhất (theo Accuracy): {best_model_name}")

print(f"Accuracy: {results_df.iloc[0]['Accuracy']:.4f}")
print(f"Precision: {results_df.iloc[0]['Precision']:.4f}")
print(f"Recall: {results_df.iloc[0]['Recall']:.4f}")
print(f"F1-Score: {results_df.iloc[0]['F1-Score']:.4f}")

"""#### Fine tuning - Hyperparameter Optimization"""

# Lấy top 3 models
top_3_models = results_df.head(3)['Model'].tolist()

# Fine tuning top 3 models
tuned_models = {}

for model_name in top_3_models:
    if model_name in param_grids and model_name in models:
        print(f"Fine-tuning {model_name}")

        random_search = RandomizedSearchCV(
            models[model_name],
            param_grids[model_name],
            n_iter=10,
            cv=3,
            scoring='f1',
            random_state=SEED,
            n_jobs=-1
        )

        random_search.fit(X_train_mice, y_train_mice)
        tuned_models[model_name] = random_search.best_estimator_

        print(f"Best parameters: {random_search.best_params_}")
        print(f"Best CV score: {random_search.best_score_:.4f}")

print(f"\nFine-tuning completed for {len(tuned_models)} models!")

"""#### Dense Stacking Ensemble (DSE) Model

**Building Dense Stacking Ensemble (DSE) Model**
"""

# Sử dụng các tuned models hoặc original models
base_models_for_ensemble = []

for model_name, model in models.items():
    if model_name in tuned_models:
        base_models_for_ensemble.append((model_name, tuned_models[model_name]))
    else:
        base_models_for_ensemble.append((model_name, model))

# Xác định meta-classifier (best model)
# Theo paper, Random Forest thường là best performer
meta_classifier = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier = tuned_models[best_model_name]

print(f"Meta-classifier: {best_model_name}")
print(f"Base models: {len(base_models_for_ensemble)}")

# 1. VOTING ENSEMBLE
print("--- Building Voting Ensemble ---")

# Create a list of estimators specifically for VotingClassifier, excluding NGBoost
# as it seems to be causing a compatibility issue with sklearn's VotingClassifier.
base_models_for_voting = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

voting_ensemble = VotingClassifier(
    estimators=base_models_for_voting,
    voting='soft'
)

voting_ensemble.fit(X_train_mice, y_train_mice)
voting_pred = voting_ensemble.predict(X_test_mice)

print("Evaluation of the Voting Ensemble")
print(f"Accuracy: {accuracy_score(y_test_mice, voting_pred):.4f}")
print(f"Precision: {precision_score(y_test_mice, voting_pred):.4f}")
print(f"Recall: {recall_score(y_test_mice, voting_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_mice, voting_pred):.4f}")

# 2. BLENDING ENSEMBLE (Stacking with meta-classifier)
print("--- Building Blending Ensemble ---")

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

blending_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier,
    cv=5
)

blending_ensemble.fit(X_train_mice, y_train_mice)
blending_pred = blending_ensemble.predict(X_test_mice)

print("Evaluation of the Blending Ensemble")
print(f"Accuracy: {accuracy_score(y_test_mice, blending_pred):.4f}")
print(f"Precision: {precision_score(y_test_mice, blending_pred):.4f}")
print(f"Recall: {recall_score(y_test_mice, blending_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_mice, blending_pred):.4f}")

# 3. FUSION ENSEMBLE (Combining base models and best model)
print("--- Building Fusion Ensemble ---")
# Train meta-classifier riêng
meta_classifier_fusion = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier_fusion = tuned_models[best_model_name]

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

fusion_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier_fusion,
    cv=5,
    passthrough=True
)

fusion_ensemble.fit(X_train_mice, y_train_mice)
fusion_pred = fusion_ensemble.predict(X_test_mice)

print("Evaluation of the Fusion Ensemble")
print(f"Accuracy: {accuracy_score(y_test_mice, fusion_pred):.4f}")
print(f"Precision: {precision_score(y_test_mice, fusion_pred):.4f}")
print(f"Recall: {recall_score(y_test_mice, fusion_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_mice, fusion_pred):.4f}")

# 4. DENSE STACKING ENSEMBLE (DSE) - Final Model
print("--- Building Dense Stacking Ensemble (DSE) ---")

# DSE combines all three approaches
dse_base_models = [
    ('voting', voting_ensemble),
    ('blending', blending_ensemble),
    ('fusion', fusion_ensemble)
]

dse_model = StackingClassifier(
    estimators=dse_base_models,
    final_estimator=meta_classifier,
    cv=5
)

dse_model.fit(X_train_mean, y_train_mean)



"""#### Stroke prediction: Final Evaluation"""

# Predictions
y_pred_dse = dse_model.predict(X_test_mice)
y_pred_proba_dse = dse_model.predict_proba(X_test_mice)[:, 1]

# Calculate metrics
dse_accuracy = accuracy_score(y_test_mice, y_pred_dse)
dse_precision = precision_score(y_test_mice, y_pred_dse, zero_division=0)
dse_recall = recall_score(y_test_mice, y_pred_dse, zero_division=0)
dse_f1 = f1_score(y_test_mice, y_pred_dse, zero_division=0)
dse_auc = roc_auc_score(y_test_mice, y_pred_proba_dse)

print("DSE MODEL PERFORMANCE:")
print(f"Accuracy:  {dse_accuracy:.4f} ({dse_accuracy*100:.2f}%)")
print(f"Precision: {dse_precision:.4f} ({dse_precision*100:.2f}%)")
print(f"Recall:    {dse_recall:.4f} ({dse_recall*100:.2f}%)")
print(f"F1-Score:  {dse_f1:.4f} ({dse_f1*100:.2f}%)")
print(f"AUC:       {dse_auc:.4f} ({dse_auc*100:.2f}%)")

# Confusion Matrix
cm = confusion_matrix(y_test_mice, y_pred_dse)
print("Confusion Matrix:")
print(cm)

visualize_confusion_matrix(cm)

plot_roc_curve(y_test_mice, y_pred_proba_dse, dse_auc)

"""#### Save the final model with the data removed for missing values."""

# Tạo folder lưu model
folder_name = 'Model for Mice Imputation Imbalanced'
os.makedirs(folder_name, exist_ok=True)

# Lưu model
model_filename = 'dse_stroke_prediction_imbalanced_mice.pkl'

model_path = os.path.join(folder_name, model_filename)
joblib.dump(dse_model, model_path)
print(f"Model saved at: '{model_path}'")

# Lưu scaler
scaler_filename = 'scaler_imbalanced_mice.pkl'
scaler_path = os.path.join(folder_name, scaler_filename)
joblib.dump(scaler, scaler_path)
print(f"Scaler saved at: '{scaler_path}'")

# Lưu encoder
encoder_filename = 'encoder_imbalanced_mice.pkl'
encoder_path = os.path.join(folder_name, encoder_filename)
joblib.dump(encoder, encoder_path)
print(f"Encoder saved at: '{encoder_path}'")

# Lưu danh sách các cột feature
model_columns_filename = 'model_columns_imbalanced_mice.pkl'
columns_path = os.path.join(folder_name, model_columns_filename)
joblib.dump(X_train_drop.columns, columns_path)
print(f"Model columns saved at: '{columns_path}'")

"""### Age Group-based Mean Imputation & Data Split"""

# Tách features và target
X_age_group = df_age_group.drop('stroke', axis=1)
y_age_group = df_age_group['stroke']

# Data Split
X_train_age_group, X_test_age_group, y_train_age_group, y_test_age_group = train_test_split(
    X_age_group, y_age_group,
    test_size=0.3,
    random_state=SEED,
    stratify=y_age_group
)

"""#### Baseline Model: Logistic Regression"""

baseline_model = LogisticRegression(random_state=SEED, max_iter=1000)
baseline_results = evaluate_model_kfold(
    baseline_model, X_train_age_group, y_train_age_group,
    k=k_fold, model_name="Baseline LR"
)

print("Baseline Model Results (K-Fold CV):")
for key, value in baseline_results.items():
    if key != 'Model':
        print(f"{key}: {value:.4f}")

# Test trên test set
baseline_model.fit(X_train_age_group, y_train_age_group)
y_pred_baseline = baseline_model.predict(X_test_age_group)

print(f"Baseline Model Test Results:")
print(f"Accuracy: {accuracy_score(y_test_age_group, y_pred_baseline):.4f}")
print(f"Precision: {precision_score(y_test_age_group, y_pred_baseline, zero_division=0):.4f}")
print(f"Recall: {recall_score(y_test_age_group, y_pred_baseline, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(y_test_age_group, y_pred_baseline, zero_division=0):.4f}")



"""#### Advanced Classification Models"""

# Train và evaluate tất cả models
all_results = []

for model_name, model in models.items():
    print('-' * 30)
    print(f"Training {model_name}")
    results = evaluate_model_kfold(model, X_train_age_group, y_train_age_group,
                                   k=k_fold, model_name=model_name)

    all_results.append(results)
    print(f"Accuracy: {results['Accuracy']:.4f}")
    print(f"Precision: {results['Precision']:.4f}")
    print(f"Recall: {results['Recall']:.4f}")
    print(f"F1-Score: {results['F1-Score']:.4f}\n")

# Thêm baseline vào kết quả
all_results.insert(0, baseline_results)

"""#### Analysis for best model"""

# Tạo DataFrame để so sánh
results_df = pd.DataFrame(all_results)
results_df = results_df.sort_values('Accuracy', ascending=False)

print("Xếp hạng Model Performance (theo Accuracy):")
print(results_df.to_string(index=False))

# Visualize kết quả
visualize_results(results_df)

# Xác định best model
best_model_name = results_df.iloc[0]['Model']
print(f"Model tốt nhất (theo Accuracy): {best_model_name}")

print(f"Accuracy: {results_df.iloc[0]['Accuracy']:.4f}")
print(f"Precision: {results_df.iloc[0]['Precision']:.4f}")
print(f"Recall: {results_df.iloc[0]['Recall']:.4f}")
print(f"F1-Score: {results_df.iloc[0]['F1-Score']:.4f}")

"""#### Fine tuning - Hyperparameter Optimization"""

# Lấy top 3 models
top_3_models = results_df.head(3)['Model'].tolist()

# Fine tuning top 3 models
tuned_models = {}

for model_name in top_3_models:
    if model_name in param_grids and model_name in models:
        print(f"Fine-tuning {model_name}")

        random_search = RandomizedSearchCV(
            models[model_name],
            param_grids[model_name],
            n_iter=10,
            cv=3,
            scoring='f1',
            random_state=SEED,
            n_jobs=-1
        )

        random_search.fit(X_train_age_group, y_train_age_group)
        tuned_models[model_name] = random_search.best_estimator_

        print(f"Best parameters: {random_search.best_params_}")
        print(f"Best CV score: {random_search.best_score_:.4f}")

print(f"\nFine-tuning completed for {len(tuned_models)} models!")

"""#### Dense Stacking Ensemble (DSE) Model

**Building Dense Stacking Ensemble (DSE) Model**
"""

# Sử dụng các tuned models hoặc original models
base_models_for_ensemble = []

for model_name, model in models.items():
    if model_name in tuned_models:
        base_models_for_ensemble.append((model_name, tuned_models[model_name]))
    else:
        base_models_for_ensemble.append((model_name, model))

# Xác định meta-classifier (best model)
# Theo paper, Random Forest thường là best performer
meta_classifier = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier = tuned_models[best_model_name]

print(f"Meta-classifier: {best_model_name}")
print(f"Base models: {len(base_models_for_ensemble)}")

# 1. VOTING ENSEMBLE
print("--- Building Voting Ensemble ---")

# Create a list of estimators specifically for VotingClassifier, excluding NGBoost
# as it seems to be causing a compatibility issue with sklearn's VotingClassifier.
base_models_for_voting = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

voting_ensemble = VotingClassifier(
    estimators=base_models_for_voting,
    voting='soft'
)

voting_ensemble.fit(X_train_age_group, y_train_age_group)
voting_pred = voting_ensemble.predict(X_test_age_group)

print("Evaluation of the Voting Ensemble")
print(f"Accuracy: {accuracy_score(y_test_age_group, voting_pred):.4f}")
print(f"Precision: {precision_score(y_test_age_group, voting_pred):.4f}")
print(f"Recall: {recall_score(y_test_age_group, voting_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_age_group, voting_pred):.4f}")

# 2. BLENDING ENSEMBLE (Stacking with meta-classifier)
print("--- Building Blending Ensemble ---")

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

blending_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier,
    cv=5
)

blending_ensemble.fit(X_train_age_group, y_train_age_group)
blending_pred = blending_ensemble.predict(X_test_age_group)

print("Evaluation of the Blending Ensemble")
print(f"Accuracy: {accuracy_score(y_test_age_group, blending_pred):.4f}")
print(f"Precision: {precision_score(y_test_age_group, blending_pred):.4f}")
print(f"Recall: {recall_score(y_test_age_group, blending_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_age_group, blending_pred):.4f}")

# 3. FUSION ENSEMBLE (Combining base models and best model)
print("--- Building Fusion Ensemble ---")
# Train meta-classifier riêng
meta_classifier_fusion = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier_fusion = tuned_models[best_model_name]

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

fusion_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier_fusion,
    cv=5,
    passthrough=True
)

fusion_ensemble.fit(X_train_age_group, y_train_age_group)
fusion_pred = fusion_ensemble.predict(X_test_age_group)

print("Evaluation of the Fusion Ensemble")
print(f"Accuracy: {accuracy_score(y_test_age_group, fusion_pred):.4f}")
print(f"Precision: {precision_score(y_test_age_group, fusion_pred):.4f}")
print(f"Recall: {recall_score(y_test_age_group, fusion_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_age_group, fusion_pred):.4f}")

# 4. DENSE STACKING ENSEMBLE (DSE) - Final Model
print("--- Building Dense Stacking Ensemble (DSE) ---")

# DSE combines all three approaches
dse_base_models = [
    ('voting', voting_ensemble),
    ('blending', blending_ensemble),
    ('fusion', fusion_ensemble)
]

dse_model = StackingClassifier(
    estimators=dse_base_models,
    final_estimator=meta_classifier,
    cv=5
)

dse_model.fit(X_train_age_group, y_train_age_group)



"""#### Stroke prediction: Final Evaluation"""

# Predictions
y_pred_dse = dse_model.predict(X_test_age_group)
y_pred_proba_dse = dse_model.predict_proba(X_test_age_group)[:, 1]

# Calculate metrics
dse_accuracy = accuracy_score(y_test_age_group, y_pred_dse)
dse_precision = precision_score(y_test_age_group, y_pred_dse, zero_division=0)
dse_recall = recall_score(y_test_age_group, y_pred_dse, zero_division=0)
dse_f1 = f1_score(y_test_age_group, y_pred_dse, zero_division=0)
dse_auc = roc_auc_score(y_test_age_group, y_pred_proba_dse)

print("DSE MODEL PERFORMANCE:")
print(f"Accuracy:  {dse_accuracy:.4f} ({dse_accuracy*100:.2f}%)")
print(f"Precision: {dse_precision:.4f} ({dse_precision*100:.2f}%)")
print(f"Recall:    {dse_recall:.4f} ({dse_recall*100:.2f}%)")
print(f"F1-Score:  {dse_f1:.4f} ({dse_f1*100:.2f}%)")
print(f"AUC:       {dse_auc:.4f} ({dse_auc*100:.2f}%)")

# Confusion Matrix
cm = confusion_matrix(y_test_age_group, y_pred_dse)
print("Confusion Matrix:")
print(cm)

visualize_confusion_matrix(cm)

plot_roc_curve(y_test_age_group, y_pred_proba_dse, dse_auc)

"""#### Save the final model with the data removed for missing values."""

# Tạo folder lưu model
folder_name = 'Model for Age Group Imputation Imbalanced'
os.makedirs(folder_name, exist_ok=True)

# Lưu model
model_filename = 'dse_stroke_prediction_imbalancedy_age_group.pkl'

model_path = os.path.join(folder_name, model_filename)
joblib.dump(dse_model, model_path)
print(f"Model saved at: '{model_path}'")

# Lưu scaler
scaler_filename = 'scaler_imbalanced_age_group.pkl'
scaler_path = os.path.join(folder_name, scaler_filename)
joblib.dump(scaler, scaler_path)
print(f"Scaler saved at: '{scaler_path}'")

# Lưu encoder
encoder_filename = 'encoder_imbalanced_age_group.pkl'
encoder_path = os.path.join(folder_name, encoder_filename)
joblib.dump(encoder, encoder_path)
print(f"Encoder saved at: '{encoder_path}'")

# Lưu danh sách các cột feature
model_columns_filename = 'model_columns_imbalanced_age_group.pkl'
columns_path = os.path.join(folder_name, model_columns_filename)
joblib.dump(X_train_drop.columns, columns_path)
print(f"Model columns saved at: '{columns_path}'")

"""### Augmented Data Split"""

# Tách features và target
X_augmented = augmented_dataset.drop('stroke', axis=1)
y_augmented = augmented_dataset['stroke']

# Data Split
X_train_augmented, X_test_augmented, y_train_augmented, y_test_augmented = train_test_split(
    X_augmented, y_augmented,
    test_size=0.3,
    random_state=SEED,
    stratify=y_augmented
)

"""#### Baseline Model: Logistic Regression"""

baseline_model = LogisticRegression(random_state=SEED, max_iter=1000)
baseline_results = evaluate_model_kfold(
    baseline_model, X_train_augmented, y_train_augmented,
    k=k_fold, model_name="Baseline LR"
)

print("Baseline Model Results (K-Fold CV):")
for key, value in baseline_results.items():
    if key != 'Model':
        print(f"{key}: {value:.4f}")

# Test trên test set
baseline_model.fit(X_train_augmented, y_train_augmented)
y_pred_baseline = baseline_model.predict(X_test_augmented)

print(f"Baseline Model Test Results:")
print(f"Accuracy: {accuracy_score(y_test_augmented, y_pred_baseline):.4f}")
print(f"Precision: {precision_score(y_test_augmented, y_pred_baseline, zero_division=0):.4f}")
print(f"Recall: {recall_score(y_test_augmented, y_pred_baseline, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(y_test_augmented, y_pred_baseline, zero_division=0):.4f}")



"""#### Advanced Classification Models"""

# Train và evaluate tất cả models
all_results = []

for model_name, model in models.items():
    print('-' * 30)
    print(f"Training {model_name}")
    results = evaluate_model_kfold(model, X_train_augmented, y_train_augmented,
                                   k=k_fold, model_name=model_name)

    all_results.append(results)
    print(f"Accuracy: {results['Accuracy']:.4f}")
    print(f"Precision: {results['Precision']:.4f}")
    print(f"Recall: {results['Recall']:.4f}")
    print(f"F1-Score: {results['F1-Score']:.4f}\n")

# Thêm baseline vào kết quả
all_results.insert(0, baseline_results)

"""#### Analysis for best model"""

# Tạo DataFrame để so sánh
results_df = pd.DataFrame(all_results)
results_df = results_df.sort_values('Accuracy', ascending=False)

print("Xếp hạng Model Performance (theo Accuracy):")
print(results_df.to_string(index=False))

# Visualize kết quả
visualize_results(results_df)

# Xác định best model
best_model_name = results_df.iloc[0]['Model']
print(f"Model tốt nhất (theo Accuracy): {best_model_name}")

print(f"Accuracy: {results_df.iloc[0]['Accuracy']:.4f}")
print(f"Precision: {results_df.iloc[0]['Precision']:.4f}")
print(f"Recall: {results_df.iloc[0]['Recall']:.4f}")
print(f"F1-Score: {results_df.iloc[0]['F1-Score']:.4f}")

"""#### Fine tuning - Hyperparameter Optimization"""

# Lấy top 3 models
top_3_models = results_df.head(3)['Model'].tolist()

# Fine tuning top 3 models
tuned_models = {}

for model_name in top_3_models:
    if model_name in param_grids and model_name in models:
        print(f"Fine-tuning {model_name}")

        random_search = RandomizedSearchCV(
            models[model_name],
            param_grids[model_name],
            n_iter=10,
            cv=3,
            scoring='f1',
            random_state=SEED,
            n_jobs=-1
        )

        random_search.fit(X_train_augmented, y_train_augmented)
        tuned_models[model_name] = random_search.best_estimator_

        print(f"Best parameters: {random_search.best_params_}")
        print(f"Best CV score: {random_search.best_score_:.4f}")

print(f"\nFine-tuning completed for {len(tuned_models)} models!")

"""#### Dense Stacking Ensemble (DSE) Model

**Building Dense Stacking Ensemble (DSE) Model**
"""

# Sử dụng các tuned models hoặc original models
base_models_for_ensemble = []

for model_name, model in models.items():
    if model_name in tuned_models:
        base_models_for_ensemble.append((model_name, tuned_models[model_name]))
    else:
        base_models_for_ensemble.append((model_name, model))

# Xác định meta-classifier (best model)
# Theo paper, Random Forest thường là best performer
meta_classifier = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier = tuned_models[best_model_name]

print(f"Meta-classifier: {best_model_name}")
print(f"Base models: {len(base_models_for_ensemble)}")

# 1. VOTING ENSEMBLE
print("--- Building Voting Ensemble ---")

# Create a list of estimators specifically for VotingClassifier, excluding NGBoost
# as it seems to be causing a compatibility issue with sklearn's VotingClassifier.
base_models_for_voting = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

voting_ensemble = VotingClassifier(
    estimators=base_models_for_voting,
    voting='soft'
)

voting_ensemble.fit(X_train_augmented, y_train_augmented)
voting_pred = voting_ensemble.predict(X_test_augmented)

print("Evaluation of the Voting Ensemble")
print(f"Accuracy: {accuracy_score(y_test_augmented, voting_pred):.4f}")
print(f"Precision: {precision_score(y_test_augmented, voting_pred):.4f}")
print(f"Recall: {recall_score(y_test_augmented, voting_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_augmented, voting_pred):.4f}")

# 2. BLENDING ENSEMBLE (Stacking with meta-classifier)
print("--- Building Blending Ensemble ---")

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

blending_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier,
    cv=5
)

blending_ensemble.fit(X_train_augmented, y_train_augmented)
blending_pred = blending_ensemble.predict(X_test_augmented)

print("Evaluation of the Blending Ensemble")
print(f"Accuracy: {accuracy_score(y_test_augmented, blending_pred):.4f}")
print(f"Precision: {precision_score(y_test_augmented, blending_pred):.4f}")
print(f"Recall: {recall_score(y_test_augmented, blending_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_augmented, blending_pred):.4f}")

# 3. FUSION ENSEMBLE (Combining base models and best model)
print("--- Building Fusion Ensemble ---")
# Train meta-classifier riêng
meta_classifier_fusion = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier_fusion = tuned_models[best_model_name]

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

fusion_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier_fusion,
    cv=5,
    passthrough=True
)

fusion_ensemble.fit(X_train_augmented, y_train_augmented)
fusion_pred = fusion_ensemble.predict(X_test_augmented)

print("Evaluation of the Fusion Ensemble")
print(f"Accuracy: {accuracy_score(y_test_augmented, fusion_pred):.4f}")
print(f"Precision: {precision_score(y_test_augmented, fusion_pred):.4f}")
print(f"Recall: {recall_score(y_test_augmented, fusion_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_augmented, fusion_pred):.4f}")

# 4. DENSE STACKING ENSEMBLE (DSE) - Final Model
print("--- Building Dense Stacking Ensemble (DSE) ---")

# DSE combines all three approaches
dse_base_models = [
    ('voting', voting_ensemble),
    ('blending', blending_ensemble),
    ('fusion', fusion_ensemble)
]

dse_model = StackingClassifier(
    estimators=dse_base_models,
    final_estimator=meta_classifier,
    cv=5
)

dse_model.fit(X_train_augmented, y_train_augmented)



"""#### Stroke prediction: Final Evaluation"""

# Predictions
y_pred_dse = dse_model.predict(X_test_augmented)
y_pred_proba_dse = dse_model.predict_proba(X_test_augmented)[:, 1]

# Calculate metrics
dse_accuracy = accuracy_score(y_test_augmented, y_pred_dse)
dse_precision = precision_score(y_test_augmented, y_pred_dse, zero_division=0)
dse_recall = recall_score(y_test_augmented, y_pred_dse, zero_division=0)
dse_f1 = f1_score(y_test_augmented, y_pred_dse, zero_division=0)
dse_auc = roc_auc_score(y_test_augmented, y_pred_proba_dse)

print("DSE MODEL PERFORMANCE:")
print(f"Accuracy:  {dse_accuracy:.4f} ({dse_accuracy*100:.2f}%)")
print(f"Precision: {dse_precision:.4f} ({dse_precision*100:.2f}%)")
print(f"Recall:    {dse_recall:.4f} ({dse_recall*100:.2f}%)")
print(f"F1-Score:  {dse_f1:.4f} ({dse_f1*100:.2f}%)")
print(f"AUC:       {dse_auc:.4f} ({dse_auc*100:.2f}%)")

# Confusion Matrix
cm = confusion_matrix(y_test_augmented, y_pred_dse)
print("Confusion Matrix:")
print(cm)

visualize_confusion_matrix(cm)

plot_roc_curve(y_test_augmented, y_pred_proba_dse, dse_auc)

"""#### Save the final model with the data removed for missing values."""

# Tạo folder lưu model
folder_name = 'Model for Augmented Imbalanced Dataset'
os.makedirs(folder_name, exist_ok=True)

# Lưu model
model_filename = 'dse_stroke_prediction_imbalanced_augmented.pkl'

model_path = os.path.join(folder_name, model_filename)
joblib.dump(dse_model, model_path)
print(f"Model saved at: '{model_path}'")

# Lưu scaler
scaler_filename = 'scaler_imbalanced_augmented.pkl'
scaler_path = os.path.join(folder_name, scaler_filename)
joblib.dump(scaler, scaler_path)
print(f"Scaler saved at: '{scaler_path}'")

# Lưu encoder
encoder_filename = 'encoder_imbalanced_augmented.pkl'
encoder_path = os.path.join(folder_name, encoder_filename)
joblib.dump(encoder, encoder_path)
print(f"Encoder saved at: '{encoder_path}'")

# Lưu danh sách các cột feature
model_columns_filename = 'model_columns_imbalanced_augmented.pkl'
columns_path = os.path.join(folder_name, model_columns_filename)
joblib.dump(X_train_drop.columns, columns_path)
print(f"Model columns saved at: '{columns_path}'")





"""## SMOTE processing

### Drop Missing Value & Data Split
"""

# Tách features và target
X_drop = df_drop.drop('stroke', axis=1)
y_drop = df_drop['stroke']

# Data Split
X_train_drop, X_test_drop, y_train_drop, y_test_drop = train_test_split(
    X_drop, y_drop,
    test_size=0.3,
    random_state=SEED,
    stratify=y_drop
)

smote = BorderlineSMOTE(random_state=SEED)

# Áp dụng BorderlineSMOTE lên tập dữ liệu huấn luyện
X_train_drop, y_train_drop = smote.fit_resample(X_train_drop, y_train_drop)

"""#### Baseline Model: Logistic Regression"""

baseline_model = LogisticRegression(random_state=SEED, max_iter=1000)
baseline_results = evaluate_model_kfold(
    baseline_model, X_train_drop, y_train_drop,
    k=k_fold, model_name="Baseline LR"
)

print("Baseline Model Results (K-Fold CV):")
for key, value in baseline_results.items():
    if key != 'Model':
        print(f"{key}: {value:.4f}")

# Test trên test set
baseline_model.fit(X_train_drop, y_train_drop)
y_pred_baseline = baseline_model.predict(X_test_drop)

print(f"Baseline Model Test Results:")
print(f"Accuracy: {accuracy_score(y_test_drop, y_pred_baseline):.4f}")
print(f"Precision: {precision_score(y_test_drop, y_pred_baseline, zero_division=0):.4f}")
print(f"Recall: {recall_score(y_test_drop, y_pred_baseline, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(y_test_drop, y_pred_baseline, zero_division=0):.4f}")



"""#### Advanced Classification Models"""

# Dictionary chứa các models
models = {
    'LR-AGD': LogisticRegression(solver='saga', max_iter=100, random_state=SEED),

    'Neural Network': MLPClassifier(
        hidden_layer_sizes=(24, 36, 48, 36, 24),  # Như paper mô tả
        random_state=SEED,
        max_iter=500
    ),

    'Random Forest': RandomForestClassifier(
        n_estimators=100,  # N_RF = 100 như paper
        random_state=SEED
    ),

    'Gradient Boosting': GradientBoostingClassifier(
        n_estimators=100,  # N_GB = 100 như paper
        random_state=SEED
    ),

    'CatBoost': CatBoostClassifier(
        iterations=100,  # N_CB = 100 như paper
        random_state=SEED,
        verbose=0
    ),

    'LightGBM': LGBMClassifier(
        n_estimators=100,  # N_LGBM = 100 như paper
        random_state=SEED,
        verbose=-1
    ),

    'XGBoost': XGBClassifier(
        n_estimators=100,  # N_XGB = 100 như paper
        random_state=SEED,
        eval_metric='logloss'
    ),

    'Balanced Bagging': BalancedBaggingClassifier(
        estimator=RandomForestClassifier(random_state=42),
        n_estimators=5,  # 5 Random Forest Classifiers như paper
        random_state=SEED
    ),

    'NGBoost': NGBClassifier(
        n_estimators=100,  # N_NGB = 100 như paper
        learning_rate=0.01,  # Như paper
        random_state=SEED,
        verbose=False
    )
}

# Train và evaluate tất cả models
all_results = []

for model_name, model in models.items():
    print('-' * 30)
    print(f"Training {model_name}")
    results = evaluate_model_kfold(model, X_train_drop, y_train_drop,
                                   k=k_fold, model_name=model_name)

    all_results.append(results)
    print(f"Accuracy: {results['Accuracy']:.4f}")
    print(f"Precision: {results['Precision']:.4f}")
    print(f"Recall: {results['Recall']:.4f}")
    print(f"F1-Score: {results['F1-Score']:.4f}\n")

# Thêm baseline vào kết quả
all_results.insert(0, baseline_results)

"""#### Analysis for best model"""

# Tạo DataFrame để so sánh
results_df = pd.DataFrame(all_results)
results_df = results_df.sort_values('Accuracy', ascending=False)

print("Xếp hạng Model Performance (theo Accuracy):")
print(results_df.to_string(index=False))

# Visualize kết quả
visualize_results(results_df)

# Xác định best model
best_model_name = results_df.iloc[0]['Model']
print(f"Model tốt nhất (theo Accuracy): {best_model_name}")

print(f"Accuracy: {results_df.iloc[0]['Accuracy']:.4f}")
print(f"Precision: {results_df.iloc[0]['Precision']:.4f}")
print(f"Recall: {results_df.iloc[0]['Recall']:.4f}")
print(f"F1-Score: {results_df.iloc[0]['F1-Score']:.4f}")

"""#### Fine tuning - Hyperparameter Optimization"""

# Lấy top 3 models
top_3_models = results_df.head(3)['Model'].tolist()

# Fine tuning top 3 models
tuned_models = {}

for model_name in top_3_models:
    if model_name in param_grids and model_name in models:
        print(f"Fine-tuning {model_name}")

        random_search = RandomizedSearchCV(
            models[model_name],
            param_grids[model_name],
            n_iter=10,
            cv=3,
            scoring='f1',
            random_state=SEED,
            n_jobs=-1
        )

        random_search.fit(X_train_drop, y_train_drop)
        tuned_models[model_name] = random_search.best_estimator_

        print(f"Best parameters: {random_search.best_params_}")
        print(f"Best CV score: {random_search.best_score_:.4f}")

print(f"\nFine-tuning completed for {len(tuned_models)} models!")

"""#### Dense Stacking Ensemble (DSE) Model

**Building Dense Stacking Ensemble (DSE) Model**
"""

# Sử dụng các tuned models hoặc original models
base_models_for_ensemble = []

for model_name, model in models.items():
    if model_name in tuned_models:
        base_models_for_ensemble.append((model_name, tuned_models[model_name]))
    else:
        base_models_for_ensemble.append((model_name, model))

# Xác định meta-classifier (best model)
# Theo paper, Random Forest thường là best performer
meta_classifier = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier = tuned_models[best_model_name]

print(f"Meta-classifier: {best_model_name}")
print(f"Base models: {len(base_models_for_ensemble)}")

# 1. VOTING ENSEMBLE
print("--- Building Voting Ensemble ---")

# Create a list of estimators specifically for VotingClassifier, excluding NGBoost
# as it seems to be causing a compatibility issue with sklearn's VotingClassifier.
base_models_for_voting = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

voting_ensemble = VotingClassifier(
    estimators=base_models_for_voting,
    voting='soft'
)

voting_ensemble.fit(X_train_drop, y_train_drop)
voting_pred = voting_ensemble.predict(X_test_drop)

print("Evaluation of the Voting Ensemble")
print(f"Accuracy: {accuracy_score(y_test_drop, voting_pred):.4f}")
print(f"Precision: {precision_score(y_test_drop, voting_pred):.4f}")
print(f"Recall: {recall_score(y_test_drop, voting_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_drop, voting_pred):.4f}")

# 2. BLENDING ENSEMBLE (Stacking with meta-classifier)
print("--- Building Blending Ensemble ---")

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

blending_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier,
    cv=5
)

blending_ensemble.fit(X_train_drop, y_train_drop)
blending_pred = blending_ensemble.predict(X_test_drop)

print("Evaluation of the Blending Ensemble")
print(f"Accuracy: {accuracy_score(y_test_drop, blending_pred):.4f}")
print(f"Precision: {precision_score(y_test_drop, blending_pred):.4f}")
print(f"Recall: {recall_score(y_test_drop, blending_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_drop, blending_pred):.4f}")

# 3. FUSION ENSEMBLE (Combining base models and best model)
print("--- Building Fusion Ensemble ---")
# Train meta-classifier riêng
meta_classifier_fusion = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier_fusion = tuned_models[best_model_name]

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

fusion_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier_fusion,
    cv=5,
    passthrough=True
)

fusion_ensemble.fit(X_train_drop, y_train_drop)
fusion_pred = fusion_ensemble.predict(X_test_drop)

print("Evaluation of the Fusion Ensemble")
print(f"Accuracy: {accuracy_score(y_test_drop, fusion_pred):.4f}")
print(f"Precision: {precision_score(y_test_drop, fusion_pred):.4f}")
print(f"Recall: {recall_score(y_test_drop, fusion_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_drop, fusion_pred):.4f}")

# 4. DENSE STACKING ENSEMBLE (DSE) - Final Model
print("--- Building Dense Stacking Ensemble (DSE) ---")

# DSE combines all three approaches
dse_base_models = [
    ('voting', voting_ensemble),
    ('blending', blending_ensemble),
    ('fusion', fusion_ensemble)
]

dse_model = StackingClassifier(
    estimators=dse_base_models,
    final_estimator=meta_classifier,
    cv=5
)

dse_model.fit(X_train_drop, y_train_drop)



"""#### Stroke prediction: Final Evaluation"""

# Predictions
y_pred_dse = dse_model.predict(X_test_drop)
y_pred_proba_dse = dse_model.predict_proba(X_test_drop)[:, 1]

# Calculate metrics
dse_accuracy = accuracy_score(y_test_drop, y_pred_dse)
dse_precision = precision_score(y_test_drop, y_pred_dse, zero_division=0)
dse_recall = recall_score(y_test_drop, y_pred_dse, zero_division=0)
dse_f1 = f1_score(y_test_drop, y_pred_dse, zero_division=0)
dse_auc = roc_auc_score(y_test_drop, y_pred_proba_dse)

print("DSE MODEL PERFORMANCE:")
print(f"Accuracy:  {dse_accuracy:.4f} ({dse_accuracy*100:.2f}%)")
print(f"Precision: {dse_precision:.4f} ({dse_precision*100:.2f}%)")
print(f"Recall:    {dse_recall:.4f} ({dse_recall*100:.2f}%)")
print(f"F1-Score:  {dse_f1:.4f} ({dse_f1*100:.2f}%)")
print(f"AUC:       {dse_auc:.4f} ({dse_auc*100:.2f}%)")

# Confusion Matrix
cm = confusion_matrix(y_test_drop, y_pred_dse)
print("Confusion Matrix:")
print(cm)

visualize_confusion_matrix(cm)

plot_roc_curve(y_test_drop, y_pred_proba_dse, dse_auc)

# So sánh với tốt nhất của paper: 96,59% (Bộ dữ liệu cân bằng)
print("Model DSE đạt được:")
print(f"Accuracy: {dse_accuracy*100:.2f}%")
print(f"Precision: {dse_precision:.4f}")
print(f"Recall: {dse_recall:.4f}")
print(f"F1-Score: {dse_f1:.4f}")
print(f"AUC: {dse_auc*100:.2f}%")

"""#### Save the final model with the data removed for missing values."""

# Tạo folder lưu model
folder_name = 'Model for Drop Missing Value SMOTE'
os.makedirs(folder_name, exist_ok=True)

# Lưu model
model_filename = 'dse_stroke_prediction_imbalanced_drop.pkl'

model_path = os.path.join(folder_name, model_filename)
joblib.dump(dse_model, model_path)
print(f"Model saved at: '{model_path}'")

# Lưu scaler
scaler_filename = 'scaler_imbalanced_drop.pkl'
scaler_path = os.path.join(folder_name, scaler_filename)
joblib.dump(scaler, scaler_path)
print(f"Scaler saved at: '{scaler_path}'")

# Lưu encoder
encoder_filename = 'encoder_imbalanced_drop.pkl'
encoder_path = os.path.join(folder_name, encoder_filename)
joblib.dump(encoder, encoder_path)
print(f"Encoder saved at: '{encoder_path}'")

# Lưu danh sách các cột feature
model_columns_filename = 'model_columns_imbalanced_drop.pkl'
columns_path = os.path.join(folder_name, model_columns_filename)
joblib.dump(X_train_drop.columns, columns_path)
print(f"Model columns saved at: '{columns_path}'")



"""### Mean imputation & Data Split"""

# Tách features và target
X_mean = df_mean.drop('stroke', axis=1)
y_mean = df_mean['stroke']

# Data Split
X_train_mean, X_test_mean, y_train_mean, y_test_mean = train_test_split(
    X_mean, y_mean,
    test_size=0.3,
    random_state=SEED,
    stratify=y_mean
)

# Áp dụng BorderlineSMOTE lên tập dữ liệu huấn luyện
X_train_mean, y_train_mean = smote.fit_resample(X_train_mean, y_train_mean)

"""#### Baseline Model: Logistic Regression"""

baseline_model = LogisticRegression(random_state=SEED, max_iter=1000)
baseline_results = evaluate_model_kfold(
    baseline_model, X_train_mean, y_train_mean,
    k=k_fold, model_name="Baseline LR"
)

print("Baseline Model Results (K-Fold CV):")
for key, value in baseline_results.items():
    if key != 'Model':
        print(f"{key}: {value:.4f}")

# Test trên test set
baseline_model.fit(X_train_mean, y_train_mean)
y_pred_baseline = baseline_model.predict(X_test_mean)

print(f"Baseline Model Test Results:")
print(f"Accuracy: {accuracy_score(y_test_mean, y_pred_baseline):.4f}")
print(f"Precision: {precision_score(y_test_mean, y_pred_baseline, zero_division=0):.4f}")
print(f"Recall: {recall_score(y_test_mean, y_pred_baseline, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(y_test_mean, y_pred_baseline, zero_division=0):.4f}")



"""#### Advanced Classification Models"""

# Train và evaluate tất cả models
all_results = []

for model_name, model in models.items():
    print('-' * 30)
    print(f"Training {model_name}")
    results = evaluate_model_kfold(model, X_train_mean, y_train_mean,
                                   k=k_fold, model_name=model_name)

    all_results.append(results)
    print(f"Accuracy: {results['Accuracy']:.4f}")
    print(f"Precision: {results['Precision']:.4f}")
    print(f"Recall: {results['Recall']:.4f}")
    print(f"F1-Score: {results['F1-Score']:.4f}\n")

# Thêm baseline vào kết quả
all_results.insert(0, baseline_results)

"""#### Analysis for best model"""

# Tạo DataFrame để so sánh
results_df = pd.DataFrame(all_results)
results_df = results_df.sort_values('Accuracy', ascending=False)

print("Xếp hạng Model Performance (theo Accuracy):")
print(results_df.to_string(index=False))

# Visualize kết quả
visualize_results(results_df)

# Xác định best model
best_model_name = results_df.iloc[0]['Model']
print(f"Model tốt nhất (theo Accuracy): {best_model_name}")

print(f"Accuracy: {results_df.iloc[0]['Accuracy']:.4f}")
print(f"Precision: {results_df.iloc[0]['Precision']:.4f}")
print(f"Recall: {results_df.iloc[0]['Recall']:.4f}")
print(f"F1-Score: {results_df.iloc[0]['F1-Score']:.4f}")

"""#### Fine tuning - Hyperparameter Optimization"""

# Lấy top 3 models
top_3_models = results_df.head(3)['Model'].tolist()

# Fine tuning top 3 models
tuned_models = {}

for model_name in top_3_models:
    if model_name in param_grids and model_name in models:
        print(f"Fine-tuning {model_name}")

        random_search = RandomizedSearchCV(
            models[model_name],
            param_grids[model_name],
            n_iter=10,
            cv=3,
            scoring='f1',
            random_state=SEED,
            n_jobs=-1
        )

        random_search.fit(X_train_mean, y_train_mean)
        tuned_models[model_name] = random_search.best_estimator_

        print(f"Best parameters: {random_search.best_params_}")
        print(f"Best CV score: {random_search.best_score_:.4f}")

print(f"\nFine-tuning completed for {len(tuned_models)} models!")

"""#### Dense Stacking Ensemble (DSE) Model

**Building Dense Stacking Ensemble (DSE) Model**
"""

# Sử dụng các tuned models hoặc original models
base_models_for_ensemble = []

for model_name, model in models.items():
    if model_name in tuned_models:
        base_models_for_ensemble.append((model_name, tuned_models[model_name]))
    else:
        base_models_for_ensemble.append((model_name, model))

# Xác định meta-classifier (best model)
# Theo paper, Random Forest thường là best performer
meta_classifier = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier = tuned_models[best_model_name]

print(f"Meta-classifier: {best_model_name}")
print(f"Base models: {len(base_models_for_ensemble)}")

# 1. VOTING ENSEMBLE
print("--- Building Voting Ensemble ---")

# Create a list of estimators specifically for VotingClassifier, excluding NGBoost
# as it seems to be causing a compatibility issue with sklearn's VotingClassifier.
base_models_for_voting = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

voting_ensemble = VotingClassifier(
    estimators=base_models_for_voting,
    voting='soft'
)

voting_ensemble.fit(X_train_mean, y_train_mean)
voting_pred = voting_ensemble.predict(X_test_mean)

print("Evaluation of the Voting Ensemble")
print(f"Accuracy: {accuracy_score(y_test_mean, voting_pred):.4f}")
print(f"Precision: {precision_score(y_test_mean, voting_pred):.4f}")
print(f"Recall: {recall_score(y_test_mean, voting_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_mean, voting_pred):.4f}")

# 2. BLENDING ENSEMBLE (Stacking with meta-classifier)
print("--- Building Blending Ensemble ---")

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

blending_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier,
    cv=5
)

blending_ensemble.fit(X_train_mean, y_train_mean)
blending_pred = blending_ensemble.predict(X_test_mean)

print("Evaluation of the Blending Ensemble")
print(f"Accuracy: {accuracy_score(y_test_mean, blending_pred):.4f}")
print(f"Precision: {precision_score(y_test_mean, blending_pred):.4f}")
print(f"Recall: {recall_score(y_test_mean, blending_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_mean, blending_pred):.4f}")

# 3. FUSION ENSEMBLE (Combining base models and best model)
print("--- Building Fusion Ensemble ---")
# Train meta-classifier riêng
meta_classifier_fusion = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier_fusion = tuned_models[best_model_name]

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

fusion_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier_fusion,
    cv=5,
    passthrough=True
)

fusion_ensemble.fit(X_train_mean, y_train_mean)
fusion_pred = fusion_ensemble.predict(X_test_mean)

print("Evaluation of the Fusion Ensemble")
print(f"Accuracy: {accuracy_score(y_test_mean, fusion_pred):.4f}")
print(f"Precision: {precision_score(y_test_mean, fusion_pred):.4f}")
print(f"Recall: {recall_score(y_test_mean, fusion_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_mean, fusion_pred):.4f}")

# 4. DENSE STACKING ENSEMBLE (DSE) - Final Model
print("--- Building Dense Stacking Ensemble (DSE) ---")

# DSE combines all three approaches
dse_base_models = [
    ('voting', voting_ensemble),
    ('blending', blending_ensemble),
    ('fusion', fusion_ensemble)
]

dse_model = StackingClassifier(
    estimators=dse_base_models,
    final_estimator=meta_classifier,
    cv=5
)

dse_model.fit(X_train_mean, y_train_mean)



"""#### Stroke prediction: Final Evaluation"""

# Predictions
y_pred_dse = dse_model.predict(X_test_mean)
y_pred_proba_dse = dse_model.predict_proba(X_test_mean)[:, 1]

# Calculate metrics
dse_accuracy = accuracy_score(y_test_mean, y_pred_dse)
dse_precision = precision_score(y_test_mean, y_pred_dse, zero_division=0)
dse_recall = recall_score(y_test_mean, y_pred_dse, zero_division=0)
dse_f1 = f1_score(y_test_mean, y_pred_dse, zero_division=0)
dse_auc = roc_auc_score(y_test_mean, y_pred_proba_dse)

print("DSE MODEL PERFORMANCE:")
print(f"Accuracy:  {dse_accuracy:.4f} ({dse_accuracy*100:.2f}%)")
print(f"Precision: {dse_precision:.4f} ({dse_precision*100:.2f}%)")
print(f"Recall:    {dse_recall:.4f} ({dse_recall*100:.2f}%)")
print(f"F1-Score:  {dse_f1:.4f} ({dse_f1*100:.2f}%)")
print(f"AUC:       {dse_auc:.4f} ({dse_auc*100:.2f}%)")

# Confusion Matrix
cm = confusion_matrix(y_test_mean, y_pred_dse)
print("Confusion Matrix:")
print(cm)

visualize_confusion_matrix(cm)

plot_roc_curve(y_test_mean, y_pred_proba_dse, dse_auc)

"""#### Save the final model with the data removed for missing values."""

# Tạo folder lưu model
folder_name = 'Model for Mean Imputation SMOTE'
os.makedirs(folder_name, exist_ok=True)

# Lưu model
model_filename = 'dse_stroke_prediction_imbalanced_mean.pkl'

model_path = os.path.join(folder_name, model_filename)
joblib.dump(dse_model, model_path)
print(f"Model saved at: '{model_path}'")

# Lưu scaler
scaler_filename = 'scaler_imbalanced_mean.pkl'
scaler_path = os.path.join(folder_name, scaler_filename)
joblib.dump(scaler, scaler_path)
print(f"Scaler saved at: '{scaler_path}'")

# Lưu encoder
encoder_filename = 'encoder_imbalanced_mean.pkl'
encoder_path = os.path.join(folder_name, encoder_filename)
joblib.dump(encoder, encoder_path)
print(f"Encoder saved at: '{encoder_path}'")

# Lưu danh sách các cột feature
model_columns_filename = 'model_columns_imbalanced_mean.pkl'
columns_path = os.path.join(folder_name, model_columns_filename)
joblib.dump(X_train_drop.columns, columns_path)
print(f"Model columns saved at: '{columns_path}'")

"""### MICE imputation & Data Split"""

# Tách features và target
X_mice = df_mice.drop('stroke', axis=1)
y_mice = df_mice['stroke']

# Data Split
X_train_mice, X_test_mice, y_train_mice, y_test_mice = train_test_split(
    X_mice, y_mice,
    test_size=0.3,
    random_state=SEED,
    stratify=y_mice
)

# Áp dụng BorderlineSMOTE lên tập dữ liệu huấn luyện
X_train_mice, y_train_mice = smote.fit_resample(X_train_mice, y_train_mice)

"""#### Baseline Model: Logistic Regression"""

baseline_model = LogisticRegression(random_state=SEED, max_iter=1000)
baseline_results = evaluate_model_kfold(
    baseline_model, X_train_mice, y_train_mice,
    k=k_fold, model_name="Baseline LR"
)

print("Baseline Model Results (K-Fold CV):")
for key, value in baseline_results.items():
    if key != 'Model':
        print(f"{key}: {value:.4f}")

# Test trên test set
baseline_model.fit(X_train_mice, y_train_mice)
y_pred_baseline = baseline_model.predict(X_test_mice)

print(f"Baseline Model Test Results:")
print(f"Accuracy: {accuracy_score(y_test_mice, y_pred_baseline):.4f}")
print(f"Precision: {precision_score(y_test_mice, y_pred_baseline, zero_division=0):.4f}")
print(f"Recall: {recall_score(y_test_mice, y_pred_baseline, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(y_test_mice, y_pred_baseline, zero_division=0):.4f}")



"""#### Advanced Classification Models"""

# Train và evaluate tất cả models
all_results = []

for model_name, model in models.items():
    print('-' * 30)
    print(f"Training {model_name}")
    results = evaluate_model_kfold(model, X_train_mice, y_train_mice,
                                   k=k_fold, model_name=model_name)

    all_results.append(results)
    print(f"Accuracy: {results['Accuracy']:.4f}")
    print(f"Precision: {results['Precision']:.4f}")
    print(f"Recall: {results['Recall']:.4f}")
    print(f"F1-Score: {results['F1-Score']:.4f}\n")

# Thêm baseline vào kết quả
all_results.insert(0, baseline_results)

"""#### Analysis for best model"""

# Tạo DataFrame để so sánh
results_df = pd.DataFrame(all_results)
results_df = results_df.sort_values('Accuracy', ascending=False)

print("Xếp hạng Model Performance (theo Accuracy):")
print(results_df.to_string(index=False))

# Visualize kết quả
visualize_results(results_df)

# Xác định best model
best_model_name = results_df.iloc[0]['Model']
print(f"Model tốt nhất (theo Accuracy): {best_model_name}")

print(f"Accuracy: {results_df.iloc[0]['Accuracy']:.4f}")
print(f"Precision: {results_df.iloc[0]['Precision']:.4f}")
print(f"Recall: {results_df.iloc[0]['Recall']:.4f}")
print(f"F1-Score: {results_df.iloc[0]['F1-Score']:.4f}")

"""#### Fine tuning - Hyperparameter Optimization"""

# Lấy top 3 models
top_3_models = results_df.head(3)['Model'].tolist()

# Fine tuning top 3 models
tuned_models = {}

for model_name in top_3_models:
    if model_name in param_grids and model_name in models:
        print(f"Fine-tuning {model_name}")

        random_search = RandomizedSearchCV(
            models[model_name],
            param_grids[model_name],
            n_iter=10,
            cv=3,
            scoring='f1',
            random_state=SEED,
            n_jobs=-1
        )

        random_search.fit(X_train_mice, y_train_mice)
        tuned_models[model_name] = random_search.best_estimator_

        print(f"Best parameters: {random_search.best_params_}")
        print(f"Best CV score: {random_search.best_score_:.4f}")

print(f"\nFine-tuning completed for {len(tuned_models)} models!")

"""#### Dense Stacking Ensemble (DSE) Model

**Building Dense Stacking Ensemble (DSE) Model**
"""

# Sử dụng các tuned models hoặc original models
base_models_for_ensemble = []

for model_name, model in models.items():
    if model_name in tuned_models:
        base_models_for_ensemble.append((model_name, tuned_models[model_name]))
    else:
        base_models_for_ensemble.append((model_name, model))

# Xác định meta-classifier (best model)
# Theo paper, Random Forest thường là best performer
meta_classifier = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier = tuned_models[best_model_name]

print(f"Meta-classifier: {best_model_name}")
print(f"Base models: {len(base_models_for_ensemble)}")

# 1. VOTING ENSEMBLE
print("--- Building Voting Ensemble ---")

# Create a list of estimators specifically for VotingClassifier, excluding NGBoost
# as it seems to be causing a compatibility issue with sklearn's VotingClassifier.
base_models_for_voting = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

voting_ensemble = VotingClassifier(
    estimators=base_models_for_voting,
    voting='soft'
)

voting_ensemble.fit(X_train_mice, y_train_mice)
voting_pred = voting_ensemble.predict(X_test_mice)

print("Evaluation of the Voting Ensemble")
print(f"Accuracy: {accuracy_score(y_test_mice, voting_pred):.4f}")
print(f"Precision: {precision_score(y_test_mice, voting_pred):.4f}")
print(f"Recall: {recall_score(y_test_mice, voting_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_mice, voting_pred):.4f}")

# 2. BLENDING ENSEMBLE (Stacking with meta-classifier)
print("--- Building Blending Ensemble ---")

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

blending_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier,
    cv=5
)

blending_ensemble.fit(X_train_mice, y_train_mice)
blending_pred = blending_ensemble.predict(X_test_mice)

print("Evaluation of the Blending Ensemble")
print(f"Accuracy: {accuracy_score(y_test_mice, blending_pred):.4f}")
print(f"Precision: {precision_score(y_test_mice, blending_pred):.4f}")
print(f"Recall: {recall_score(y_test_mice, blending_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_mice, blending_pred):.4f}")

# 3. FUSION ENSEMBLE (Combining base models and best model)
print("--- Building Fusion Ensemble ---")
# Train meta-classifier riêng
meta_classifier_fusion = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier_fusion = tuned_models[best_model_name]

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

fusion_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier_fusion,
    cv=5,
    passthrough=True
)

fusion_ensemble.fit(X_train_mice, y_train_mice)
fusion_pred = fusion_ensemble.predict(X_test_mice)

print("Evaluation of the Fusion Ensemble")
print(f"Accuracy: {accuracy_score(y_test_mice, fusion_pred):.4f}")
print(f"Precision: {precision_score(y_test_mice, fusion_pred):.4f}")
print(f"Recall: {recall_score(y_test_mice, fusion_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_mice, fusion_pred):.4f}")

# 4. DENSE STACKING ENSEMBLE (DSE) - Final Model
print("--- Building Dense Stacking Ensemble (DSE) ---")

# DSE combines all three approaches
dse_base_models = [
    ('voting', voting_ensemble),
    ('blending', blending_ensemble),
    ('fusion', fusion_ensemble)
]

dse_model = StackingClassifier(
    estimators=dse_base_models,
    final_estimator=meta_classifier,
    cv=5
)

dse_model.fit(X_train_mean, y_train_mean)



"""#### Stroke prediction: Final Evaluation"""

# Predictions
y_pred_dse = dse_model.predict(X_test_mice)
y_pred_proba_dse = dse_model.predict_proba(X_test_mice)[:, 1]

# Calculate metrics
dse_accuracy = accuracy_score(y_test_mice, y_pred_dse)
dse_precision = precision_score(y_test_mice, y_pred_dse, zero_division=0)
dse_recall = recall_score(y_test_mice, y_pred_dse, zero_division=0)
dse_f1 = f1_score(y_test_mice, y_pred_dse, zero_division=0)
dse_auc = roc_auc_score(y_test_mice, y_pred_proba_dse)

print("DSE MODEL PERFORMANCE:")
print(f"Accuracy:  {dse_accuracy:.4f} ({dse_accuracy*100:.2f}%)")
print(f"Precision: {dse_precision:.4f} ({dse_precision*100:.2f}%)")
print(f"Recall:    {dse_recall:.4f} ({dse_recall*100:.2f}%)")
print(f"F1-Score:  {dse_f1:.4f} ({dse_f1*100:.2f}%)")
print(f"AUC:       {dse_auc:.4f} ({dse_auc*100:.2f}%)")

# Confusion Matrix
cm = confusion_matrix(y_test_mice, y_pred_dse)
print("Confusion Matrix:")
print(cm)

visualize_confusion_matrix(cm)

plot_roc_curve(y_test_mice, y_pred_proba_dse, dse_auc)

"""#### Save the final model with the data removed for missing values."""

# Tạo folder lưu model
folder_name = 'Model for Mice Imputation SMOTE'
os.makedirs(folder_name, exist_ok=True)

# Lưu model
model_filename = 'dse_stroke_prediction_imbalanced_mice.pkl'

model_path = os.path.join(folder_name, model_filename)
joblib.dump(dse_model, model_path)
print(f"Model saved at: '{model_path}'")

# Lưu scaler
scaler_filename = 'scaler_imbalanced_mice.pkl'
scaler_path = os.path.join(folder_name, scaler_filename)
joblib.dump(scaler, scaler_path)
print(f"Scaler saved at: '{scaler_path}'")

# Lưu encoder
encoder_filename = 'encoder_imbalanced_mice.pkl'
encoder_path = os.path.join(folder_name, encoder_filename)
joblib.dump(encoder, encoder_path)
print(f"Encoder saved at: '{encoder_path}'")

# Lưu danh sách các cột feature
model_columns_filename = 'model_columns_imbalanced_mice.pkl'
columns_path = os.path.join(folder_name, model_columns_filename)
joblib.dump(X_train_drop.columns, columns_path)
print(f"Model columns saved at: '{columns_path}'")

"""### Age Group-based Mean Imputation & Data Split"""

# Tách features và target
X_age_group = df_age_group.drop('stroke', axis=1)
y_age_group = df_age_group['stroke']

# Data Split
X_train_age_group, X_test_age_group, y_train_age_group, y_test_age_group = train_test_split(
    X_age_group, y_age_group,
    test_size=0.3,
    random_state=SEED,
    stratify=y_age_group
)

# Áp dụng BorderlineSMOTE lên tập dữ liệu huấn luyện
X_train_age_group, y_train_age_group = smote.fit_resample(X_train_age_group, y_train_age_group)

"""#### Baseline Model: Logistic Regression"""

baseline_model = LogisticRegression(random_state=SEED, max_iter=1000)
baseline_results = evaluate_model_kfold(
    baseline_model, X_train_age_group, y_train_age_group,
    k=k_fold, model_name="Baseline LR"
)

print("Baseline Model Results (K-Fold CV):")
for key, value in baseline_results.items():
    if key != 'Model':
        print(f"{key}: {value:.4f}")

# Test trên test set
baseline_model.fit(X_train_age_group, y_train_age_group)
y_pred_baseline = baseline_model.predict(X_test_age_group)

print(f"Baseline Model Test Results:")
print(f"Accuracy: {accuracy_score(y_test_age_group, y_pred_baseline):.4f}")
print(f"Precision: {precision_score(y_test_age_group, y_pred_baseline, zero_division=0):.4f}")
print(f"Recall: {recall_score(y_test_age_group, y_pred_baseline, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(y_test_age_group, y_pred_baseline, zero_division=0):.4f}")



"""#### Advanced Classification Models"""

# Train và evaluate tất cả models
all_results = []

for model_name, model in models.items():
    print('-' * 30)
    print(f"Training {model_name}")
    results = evaluate_model_kfold(model, X_train_age_group, y_train_age_group,
                                   k=k_fold, model_name=model_name)

    all_results.append(results)
    print(f"Accuracy: {results['Accuracy']:.4f}")
    print(f"Precision: {results['Precision']:.4f}")
    print(f"Recall: {results['Recall']:.4f}")
    print(f"F1-Score: {results['F1-Score']:.4f}\n")

# Thêm baseline vào kết quả
all_results.insert(0, baseline_results)

"""#### Analysis for best model"""

# Tạo DataFrame để so sánh
results_df = pd.DataFrame(all_results)
results_df = results_df.sort_values('Accuracy', ascending=False)

print("Xếp hạng Model Performance (theo Accuracy):")
print(results_df.to_string(index=False))

# Visualize kết quả
visualize_results(results_df)

# Xác định best model
best_model_name = results_df.iloc[0]['Model']
print(f"Model tốt nhất (theo Accuracy): {best_model_name}")

print(f"Accuracy: {results_df.iloc[0]['Accuracy']:.4f}")
print(f"Precision: {results_df.iloc[0]['Precision']:.4f}")
print(f"Recall: {results_df.iloc[0]['Recall']:.4f}")
print(f"F1-Score: {results_df.iloc[0]['F1-Score']:.4f}")

"""#### Fine tuning - Hyperparameter Optimization"""

# Lấy top 3 models
top_3_models = results_df.head(3)['Model'].tolist()

# Fine tuning top 3 models
tuned_models = {}

for model_name in top_3_models:
    if model_name in param_grids and model_name in models:
        print(f"Fine-tuning {model_name}")

        random_search = RandomizedSearchCV(
            models[model_name],
            param_grids[model_name],
            n_iter=10,
            cv=3,
            scoring='f1',
            random_state=SEED,
            n_jobs=-1
        )

        random_search.fit(X_train_age_group, y_train_age_group)
        tuned_models[model_name] = random_search.best_estimator_

        print(f"Best parameters: {random_search.best_params_}")
        print(f"Best CV score: {random_search.best_score_:.4f}")

print(f"\nFine-tuning completed for {len(tuned_models)} models!")

"""#### Dense Stacking Ensemble (DSE) Model

**Building Dense Stacking Ensemble (DSE) Model**
"""

# Sử dụng các tuned models hoặc original models
base_models_for_ensemble = []

for model_name, model in models.items():
    if model_name in tuned_models:
        base_models_for_ensemble.append((model_name, tuned_models[model_name]))
    else:
        base_models_for_ensemble.append((model_name, model))

# Xác định meta-classifier (best model)
# Theo paper, Random Forest thường là best performer
meta_classifier = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier = tuned_models[best_model_name]

print(f"Meta-classifier: {best_model_name}")
print(f"Base models: {len(base_models_for_ensemble)}")

# 1. VOTING ENSEMBLE
print("--- Building Voting Ensemble ---")

# Create a list of estimators specifically for VotingClassifier, excluding NGBoost
# as it seems to be causing a compatibility issue with sklearn's VotingClassifier.
base_models_for_voting = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

voting_ensemble = VotingClassifier(
    estimators=base_models_for_voting,
    voting='soft'
)

voting_ensemble.fit(X_train_age_group, y_train_age_group)
voting_pred = voting_ensemble.predict(X_test_age_group)

print("Evaluation of the Voting Ensemble")
print(f"Accuracy: {accuracy_score(y_test_age_group, voting_pred):.4f}")
print(f"Precision: {precision_score(y_test_age_group, voting_pred):.4f}")
print(f"Recall: {recall_score(y_test_age_group, voting_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_age_group, voting_pred):.4f}")

# 2. BLENDING ENSEMBLE (Stacking with meta-classifier)
print("--- Building Blending Ensemble ---")

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

blending_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier,
    cv=5
)

blending_ensemble.fit(X_train_age_group, y_train_age_group)
blending_pred = blending_ensemble.predict(X_test_age_group)

print("Evaluation of the Blending Ensemble")
print(f"Accuracy: {accuracy_score(y_test_age_group, blending_pred):.4f}")
print(f"Precision: {precision_score(y_test_age_group, blending_pred):.4f}")
print(f"Recall: {recall_score(y_test_age_group, blending_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_age_group, blending_pred):.4f}")

# 3. FUSION ENSEMBLE (Combining base models and best model)
print("--- Building Fusion Ensemble ---")
# Train meta-classifier riêng
meta_classifier_fusion = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier_fusion = tuned_models[best_model_name]

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

fusion_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier_fusion,
    cv=5,
    passthrough=True
)

fusion_ensemble.fit(X_train_age_group, y_train_age_group)
fusion_pred = fusion_ensemble.predict(X_test_age_group)

print("Evaluation of the Fusion Ensemble")
print(f"Accuracy: {accuracy_score(y_test_age_group, fusion_pred):.4f}")
print(f"Precision: {precision_score(y_test_age_group, fusion_pred):.4f}")
print(f"Recall: {recall_score(y_test_age_group, fusion_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_age_group, fusion_pred):.4f}")

# 4. DENSE STACKING ENSEMBLE (DSE) - Final Model
print("--- Building Dense Stacking Ensemble (DSE) ---")

# DSE combines all three approaches
dse_base_models = [
    ('voting', voting_ensemble),
    ('blending', blending_ensemble),
    ('fusion', fusion_ensemble)
]

dse_model = StackingClassifier(
    estimators=dse_base_models,
    final_estimator=meta_classifier,
    cv=5
)

dse_model.fit(X_train_age_group, y_train_age_group)



"""#### Stroke prediction: Final Evaluation"""

# Predictions
y_pred_dse = dse_model.predict(X_test_age_group)
y_pred_proba_dse = dse_model.predict_proba(X_test_age_group)[:, 1]

# Calculate metrics
dse_accuracy = accuracy_score(y_test_age_group, y_pred_dse)
dse_precision = precision_score(y_test_age_group, y_pred_dse, zero_division=0)
dse_recall = recall_score(y_test_age_group, y_pred_dse, zero_division=0)
dse_f1 = f1_score(y_test_age_group, y_pred_dse, zero_division=0)
dse_auc = roc_auc_score(y_test_age_group, y_pred_proba_dse)

print("DSE MODEL PERFORMANCE:")
print(f"Accuracy:  {dse_accuracy:.4f} ({dse_accuracy*100:.2f}%)")
print(f"Precision: {dse_precision:.4f} ({dse_precision*100:.2f}%)")
print(f"Recall:    {dse_recall:.4f} ({dse_recall*100:.2f}%)")
print(f"F1-Score:  {dse_f1:.4f} ({dse_f1*100:.2f}%)")
print(f"AUC:       {dse_auc:.4f} ({dse_auc*100:.2f}%)")

# Confusion Matrix
cm = confusion_matrix(y_test_age_group, y_pred_dse)
print("Confusion Matrix:")
print(cm)

visualize_confusion_matrix(cm)

plot_roc_curve(y_test_age_group, y_pred_proba_dse, dse_auc)

"""#### Save the final model with the data removed for missing values."""

# Tạo folder lưu model
folder_name = 'Model for Age Group Imputation SMOTE'
os.makedirs(folder_name, exist_ok=True)

# Lưu model
model_filename = 'dse_stroke_prediction_imbalancedy_age_group.pkl'

model_path = os.path.join(folder_name, model_filename)
joblib.dump(dse_model, model_path)
print(f"Model saved at: '{model_path}'")

# Lưu scaler
scaler_filename = 'scaler_imbalanced_age_group.pkl'
scaler_path = os.path.join(folder_name, scaler_filename)
joblib.dump(scaler, scaler_path)
print(f"Scaler saved at: '{scaler_path}'")

# Lưu encoder
encoder_filename = 'encoder_imbalanced_age_group.pkl'
encoder_path = os.path.join(folder_name, encoder_filename)
joblib.dump(encoder, encoder_path)
print(f"Encoder saved at: '{encoder_path}'")

# Lưu danh sách các cột feature
model_columns_filename = 'model_columns_imbalanced_age_group.pkl'
columns_path = os.path.join(folder_name, model_columns_filename)
joblib.dump(X_train_drop.columns, columns_path)
print(f"Model columns saved at: '{columns_path}'")

"""### Augmented Data Split"""

# Tách features và target
X_augmented = augmented_dataset.drop('stroke', axis=1)
y_augmented = augmented_dataset['stroke']

# Data Split
X_train_augmented, X_test_augmented, y_train_augmented, y_test_augmented = train_test_split(
    X_augmented, y_augmented,
    test_size=0.3,
    random_state=SEED,
    stratify=y_augmented
)

# Áp dụng BorderlineSMOTE lên tập dữ liệu huấn luyện
X_train_augmented, y_train_augmented = smote.fit_resample(X_train_augmented, y_train_augmented)

"""#### Baseline Model: Logistic Regression"""

baseline_model = LogisticRegression(random_state=SEED, max_iter=1000)
baseline_results = evaluate_model_kfold(
    baseline_model, X_train_augmented, y_train_augmented,
    k=k_fold, model_name="Baseline LR"
)

print("Baseline Model Results (K-Fold CV):")
for key, value in baseline_results.items():
    if key != 'Model':
        print(f"{key}: {value:.4f}")

# Test trên test set
baseline_model.fit(X_train_augmented, y_train_augmented)
y_pred_baseline = baseline_model.predict(X_test_augmented)

print(f"Baseline Model Test Results:")
print(f"Accuracy: {accuracy_score(y_test_augmented, y_pred_baseline):.4f}")
print(f"Precision: {precision_score(y_test_augmented, y_pred_baseline, zero_division=0):.4f}")
print(f"Recall: {recall_score(y_test_augmented, y_pred_baseline, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(y_test_augmented, y_pred_baseline, zero_division=0):.4f}")



"""#### Advanced Classification Models"""

# Train và evaluate tất cả models
all_results = []

for model_name, model in models.items():
    print('-' * 30)
    print(f"Training {model_name}")
    results = evaluate_model_kfold(model, X_train_augmented, y_train_augmented,
                                   k=k_fold, model_name=model_name)

    all_results.append(results)
    print(f"Accuracy: {results['Accuracy']:.4f}")
    print(f"Precision: {results['Precision']:.4f}")
    print(f"Recall: {results['Recall']:.4f}")
    print(f"F1-Score: {results['F1-Score']:.4f}\n")

# Thêm baseline vào kết quả
all_results.insert(0, baseline_results)

"""#### Analysis for best model"""

# Tạo DataFrame để so sánh
results_df = pd.DataFrame(all_results)
results_df = results_df.sort_values('Accuracy', ascending=False)

print("Xếp hạng Model Performance (theo Accuracy):")
print(results_df.to_string(index=False))

# Visualize kết quả
visualize_results(results_df)

# Xác định best model
best_model_name = results_df.iloc[0]['Model']
print(f"Model tốt nhất (theo Accuracy): {best_model_name}")

print(f"Accuracy: {results_df.iloc[0]['Accuracy']:.4f}")
print(f"Precision: {results_df.iloc[0]['Precision']:.4f}")
print(f"Recall: {results_df.iloc[0]['Recall']:.4f}")
print(f"F1-Score: {results_df.iloc[0]['F1-Score']:.4f}")

"""#### Fine tuning - Hyperparameter Optimization"""

# Lấy top 3 models
top_3_models = results_df.head(3)['Model'].tolist()

# Fine tuning top 3 models
tuned_models = {}

for model_name in top_3_models:
    if model_name in param_grids and model_name in models:
        print(f"Fine-tuning {model_name}")

        random_search = RandomizedSearchCV(
            models[model_name],
            param_grids[model_name],
            n_iter=10,
            cv=3,
            scoring='f1',
            random_state=SEED,
            n_jobs=-1
        )

        random_search.fit(X_train_augmented, y_train_augmented)
        tuned_models[model_name] = random_search.best_estimator_

        print(f"Best parameters: {random_search.best_params_}")
        print(f"Best CV score: {random_search.best_score_:.4f}")

print(f"\nFine-tuning completed for {len(tuned_models)} models!")

"""#### Dense Stacking Ensemble (DSE) Model

**Building Dense Stacking Ensemble (DSE) Model**
"""

# Sử dụng các tuned models hoặc original models
base_models_for_ensemble = []

for model_name, model in models.items():
    if model_name in tuned_models:
        base_models_for_ensemble.append((model_name, tuned_models[model_name]))
    else:
        base_models_for_ensemble.append((model_name, model))

# Xác định meta-classifier (best model)
# Theo paper, Random Forest thường là best performer
meta_classifier = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier = tuned_models[best_model_name]

print(f"Meta-classifier: {best_model_name}")
print(f"Base models: {len(base_models_for_ensemble)}")

# 1. VOTING ENSEMBLE
print("--- Building Voting Ensemble ---")

# Create a list of estimators specifically for VotingClassifier, excluding NGBoost
# as it seems to be causing a compatibility issue with sklearn's VotingClassifier.
base_models_for_voting = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

voting_ensemble = VotingClassifier(
    estimators=base_models_for_voting,
    voting='soft'
)

voting_ensemble.fit(X_train_augmented, y_train_augmented)
voting_pred = voting_ensemble.predict(X_test_augmented)

print("Evaluation of the Voting Ensemble")
print(f"Accuracy: {accuracy_score(y_test_augmented, voting_pred):.4f}")
print(f"Precision: {precision_score(y_test_augmented, voting_pred):.4f}")
print(f"Recall: {recall_score(y_test_augmented, voting_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_augmented, voting_pred):.4f}")

# 2. BLENDING ENSEMBLE (Stacking with meta-classifier)
print("--- Building Blending Ensemble ---")

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

blending_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier,
    cv=5
)

blending_ensemble.fit(X_train_augmented, y_train_augmented)
blending_pred = blending_ensemble.predict(X_test_augmented)

print("Evaluation of the Blending Ensemble")
print(f"Accuracy: {accuracy_score(y_test_augmented, blending_pred):.4f}")
print(f"Precision: {precision_score(y_test_augmented, blending_pred):.4f}")
print(f"Recall: {recall_score(y_test_augmented, blending_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_augmented, blending_pred):.4f}")

# 3. FUSION ENSEMBLE (Combining base models and best model)
print("--- Building Fusion Ensemble ---")
# Train meta-classifier riêng
meta_classifier_fusion = RandomForestClassifier(n_estimators=100, random_state=SEED)
if best_model_name in tuned_models:
    meta_classifier_fusion = tuned_models[best_model_name]

# Exclude NGBoost from the base models for StackingClassifier due to compatibility issues
base_models_for_stacking = [
    (name, model) for name, model in base_models_for_ensemble if name != 'NGBoost'
]

fusion_ensemble = StackingClassifier(
    estimators=base_models_for_stacking,
    final_estimator=meta_classifier_fusion,
    cv=5,
    passthrough=True
)

fusion_ensemble.fit(X_train_augmented, y_train_augmented)
fusion_pred = fusion_ensemble.predict(X_test_augmented)

print("Evaluation of the Fusion Ensemble")
print(f"Accuracy: {accuracy_score(y_test_augmented, fusion_pred):.4f}")
print(f"Precision: {precision_score(y_test_augmented, fusion_pred):.4f}")
print(f"Recall: {recall_score(y_test_augmented, fusion_pred):.4f}")
print(f"F1-Score: {f1_score(y_test_augmented, fusion_pred):.4f}")

# 4. DENSE STACKING ENSEMBLE (DSE) - Final Model
print("--- Building Dense Stacking Ensemble (DSE) ---")

# DSE combines all three approaches
dse_base_models = [
    ('voting', voting_ensemble),
    ('blending', blending_ensemble),
    ('fusion', fusion_ensemble)
]

dse_model = StackingClassifier(
    estimators=dse_base_models,
    final_estimator=meta_classifier,
    cv=5
)

dse_model.fit(X_train_augmented, y_train_augmented)



"""#### Stroke prediction: Final Evaluation"""

# Predictions
y_pred_dse = dse_model.predict(X_test_augmented)
y_pred_proba_dse = dse_model.predict_proba(X_test_augmented)[:, 1]

# Calculate metrics
dse_accuracy = accuracy_score(y_test_augmented, y_pred_dse)
dse_precision = precision_score(y_test_augmented, y_pred_dse, zero_division=0)
dse_recall = recall_score(y_test_augmented, y_pred_dse, zero_division=0)
dse_f1 = f1_score(y_test_augmented, y_pred_dse, zero_division=0)
dse_auc = roc_auc_score(y_test_augmented, y_pred_proba_dse)

print("DSE MODEL PERFORMANCE:")
print(f"Accuracy:  {dse_accuracy:.4f} ({dse_accuracy*100:.2f}%)")
print(f"Precision: {dse_precision:.4f} ({dse_precision*100:.2f}%)")
print(f"Recall:    {dse_recall:.4f} ({dse_recall*100:.2f}%)")
print(f"F1-Score:  {dse_f1:.4f} ({dse_f1*100:.2f}%)")
print(f"AUC:       {dse_auc:.4f} ({dse_auc*100:.2f}%)")

# Confusion Matrix
cm = confusion_matrix(y_test_augmented, y_pred_dse)
print("Confusion Matrix:")
print(cm)

visualize_confusion_matrix(cm)

plot_roc_curve(y_test_augmented, y_pred_proba_dse, dse_auc)

"""#### Save the final model with the data removed for missing values."""

# Tạo folder lưu model
folder_name = 'Model for Augmented SMOTE Dataset'
os.makedirs(folder_name, exist_ok=True)

# Lưu model
model_filename = 'dse_stroke_prediction_imbalanced_augmented.pkl'

model_path = os.path.join(folder_name, model_filename)
joblib.dump(dse_model, model_path)
print(f"Model saved at: '{model_path}'")

# Lưu scaler
scaler_filename = 'scaler_imbalanced_augmented.pkl'
scaler_path = os.path.join(folder_name, scaler_filename)
joblib.dump(scaler, scaler_path)
print(f"Scaler saved at: '{scaler_path}'")

# Lưu encoder
encoder_filename = 'encoder_imbalanced_augmented.pkl'
encoder_path = os.path.join(folder_name, encoder_filename)
joblib.dump(encoder, encoder_path)
print(f"Encoder saved at: '{encoder_path}'")

# Lưu danh sách các cột feature
model_columns_filename = 'model_columns_imbalanced_augmented.pkl'
columns_path = os.path.join(folder_name, model_columns_filename)
joblib.dump(X_train_drop.columns, columns_path)
print(f"Model columns saved at: '{columns_path}'")





"""### So sánh với các kết quả trong paper"""

paper_results = {
    'Study': ['Minimal Genetic Folding', 'Logistic Regression', 'Naive Bayes',
              'Ensemble Random Forest', 'SVM', 'Random Forest (1)', 'Random Forest (2)',
              'Random Forest (3)', 'Proposed RXLM', 'K-NN', 'Proposed DSE (This Study)'],
    'Accuracy': [83.2, 86.0, 82.0, 94.46, 95.49, 95.5, 96.0, 96.0, 96.34, 94.0, dse_accuracy*100],
    'Dataset': ['Balanced', 'Balanced', 'Balanced', 'Imbalanced', 'Imbalanced',
                'Imbalanced', 'Balanced', 'Balanced', 'Balanced', 'Balanced', 'Imbalanced']
}

comparison_df = pd.DataFrame(paper_results)
comparison_df = comparison_df.sort_values('Accuracy', ascending=False)

print("So sánh với các nghiên cứu trước đây:")
print(comparison_df.to_string(index=False))

# Visualize comparison
plt.figure(figsize=(12, 6))
colors = ['red' if study == 'Proposed DSE (This Study)' else 'skyblue'
          for study in comparison_df['Study']]
plt.barh(comparison_df['Study'], comparison_df['Accuracy'], color=colors)
plt.xlabel('Accuracy (%)')
plt.title('Stroke Prediction Models Comparison')
plt.axvline(x=dse_accuracy*100, color='red', linestyle='--',
            label=f'Our DSE Model: {dse_accuracy*100:.2f}%')
plt.legend()
plt.tight_layout()
plt.show()



"""#### Feature Importance Analysis"""

# Lấy feature importance từ meta-classifier nếu có
if hasattr(dse_model.final_estimator_, 'feature_importances_'):
    # Get predictions from base models
    base_predictions = []
    for name, model in dse_base_models:
        pred = model.predict_proba(X_train_drop)
        base_predictions.append(pred)

    # Feature importance từ final estimator
    importances = dse_model.final_estimator_.feature_importances_

    # Create feature names
    feature_names = [f"{name}_0" for name, _ in dse_base_models] + \
                   [f"{name}_1" for name, _ in dse_base_models]

    # Sort by importance
    indices = np.argsort(importances)[::-1]

    print("Top Features (from ensemble):")
    for i in range(min(10, len(indices))):
        print(f"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}")# Lấy feature importance từ meta-classifier nếu có
if hasattr(dse_model.final_estimator_, 'feature_importances_'):
    # Get predictions from base models
    base_predictions = []
    for name, model in dse_base_models:
        pred = model.predict_proba(X_train_drop)
        base_predictions.append(pred)

    # Feature importance từ final estimator
    importances = dse_model.final_estimator_.feature_importances_

    # Create feature names
    feature_names = [f"{name}_0" for name, _ in dse_base_models] + \
                   [f"{name}_1" for name, _ in dse_base_models]

    # Sort by importance
    indices = np.argsort(importances)[::-1]

    print("Top Features (from ensemble):")
    for i in range(min(10, len(indices))):
        print(f"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}")

# Feature importance từ original features (using Random Forest)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_drop, y_train_drop)

feature_importance = pd.DataFrame({
    'Feature': X_train_drop.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

print("Original Feature Importance (Random Forest):")
print(feature_importance)

# Visualize
plt.figure(figsize=(5, 3))
plt.barh(feature_importance['Feature'][:10], feature_importance['Importance'][:10])
plt.xlabel('Importance')
plt.title('Top 10 Most Important Features')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()# Visualize
plt.figure(figsize=(5, 3))
plt.barh(feature_importance['Feature'][:10], feature_importance['Importance'][:10])
plt.xlabel('Importance')
plt.title('Top 10 Most Important Features')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()



"""### *Implement hàm này để sau này test trên web*"""

# Function để predict cho new data
def predict_stroke(patient_data):
    """
    Predict stroke risk for a new patient

    patient_data: dict with keys matching feature names
    Returns: prediction (0/1) and probability
    """
    # Load model và scaler
    model = joblib.load(model_filename)
    scaler = joblib.load(scaler_filename)

    # Prepare data
    patient_df = pd.DataFrame([patient_data])

    # Scale numerical features
    patient_df[numerical_cols] = scaler.transform(patient_df[numerical_cols])

    # Predict
    prediction = model.predict(patient_df)[0]
    probability = model.predict_proba(patient_df)[0][1]

    return prediction, probability

